{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adaptive video\n",
    "\n",
    "*Fraida Fund ffund@nyu.edu*\n",
    "\n",
    "This experiment explores the tradeoff between different metrics of video quality (average rate, interruptions, and variability of rate) in an adaptive video delivery system.\n",
    "\n",
    "It should take about 60-120 minutes to run this experiment.\n",
    "\n",
    "**Preqreuisites**: This experiment uses both Chameleon and FABRIC! You should have a Chameleon account and be part of a Chameleon project, and you should have a FABRIC account and be part of a FABRIC project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure FABRIC on Chameleon\n",
    "\n",
    "In this experiment, you will use the JupyterHub environment on Chameleon to reserve and access resources on the FABRIC testbed. First, you will need to configure your Chameleon environment, so that it will be able to use your FABRIC account.\n",
    "\n",
    "If you’ve already done this (for example, in a different experiment!) then you don’t have to do it again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a FABRIC configuration directory\n",
    "\n",
    "First, in your Chameleon JupyterHub environment, create a directory in which to store FABRIC configuration files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "mkdir -p ~/work/fabric_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get a FABRIC token and upload it to JupyterHub\n",
    "\n",
    "Now, you will need to create a FABRIC token, if you don’t currently have an active token.\n",
    "\n",
    "In the FABRIC portal, click on Experiments \\> Manage Tokens \\> Open FABRIC Credential Manager and log in. Then, in the “Create Token” section,\n",
    "\n",
    "-   Select your FABRIC project from the drop-down menu\n",
    "-   Leave the scope to All\n",
    "-   Click the “Create Token” button\n",
    "\n",
    "Click “Download” to download the token. Rename it to `id_token.json`.\n",
    "\n",
    "In your JupyterHub environment on Chameleon, upload `id_token.json` to your `~/work/fabric_config` directory. Verify that it is there:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "[[ -f ~/work/fabric_config/id_token.json ]] && echo \"~/work/fabric_config/id_token.json exists.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure the output of the cell above says: “~/work/fabric_config/id_token.json exists.”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get a bastion keypair and upload it to JupyterHub\n",
    "\n",
    "To log in to resources on FABRIC, you will need a bastion keypair in your Chameleon JupyterHub environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you already have a FABRIC bastion keypair (public and private key), upload them to the JupyterHub environment no Chameleon. Name the private and public key `fabric_chameleon_bkey` and `fabric_chameleon_bkey.pub`, respectively, and upload them to your `~/work/fabric_config` directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you don’t already have a key pair, create one now. In the FABRIC portal, visit Experiments \\> Manage SSH Keys and scroll to the “Generate SSH Key Pair” section.\n",
    "\n",
    "Fill in details, make sure the key type is set to “bastion”, then click “Generate Key Pair”\n",
    "\n",
    "<figure>\n",
    "<img src=\"attachment:images/generate_fabric_bastion_key.png\" alt=\"Generate a bastion key pair\" />\n",
    "<figcaption aria-hidden=\"true\">Generate a bastion key pair</figcaption>\n",
    "</figure>\n",
    "\n",
    "Then, on the next screen, download both the public and private key. Upload them to your `~/work/fabric_config` directory in the Chameleon JupyterHub environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify that the bastion keypair is uploaded to the environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "[[ -f ~/work/fabric_config/fabric_chameleon_bkey ]] && echo \"~/work/fabric_config/fabric_chameleon_bkey exists.\"\n",
    "[[ -f ~/work/fabric_config/fabric_chameleon_bkey.pub ]] && echo \"~/work/fabric_config/fabric_chameleon_bkey.pub exists.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also make sure that the private key has the appropriate permissions set (not too open):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "chmod 600  ~/work/fabric_config/fabric_chameleon_bkey "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate slice keys\n",
    "\n",
    "In addition to the bastion key pair, you will also need to generate a slice key pair. You can generate a key pair with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "ssh-keygen -t rsa -b 3072 -f ${HOME}/work/fabric_config/slice_key -q -N \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also make sure that the private key has the appropriate permissions set (not too open):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "chmod 600  ~/work/fabric_config/slice_key "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up environment variables and generate configuration files\n",
    "\n",
    "Now that all of the credential files we need exist in the Chameleon JupyterHub environment, we can set up configuration files.\n",
    "\n",
    "In the cell below, you will need to substitute your own FABRIC bastion username and FABRIC project ID in the places where it says `xxx`.\n",
    "\n",
    "-   You can find your bastion username in the FABRIC portal, under “User Profile” - scroll to the table at the bottom of that page and look for the value next to “Bastion login”.\n",
    "-   You can find your project ID in the FABRIC Portal, under “Projects” - click on the project that you want to use, scroll to the table at the bottom of that page, and look for the value next to “Project ID”.\n",
    "\n",
    "Then, this cell will generate configuration files with your FABRIC settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "\n",
    "# these two variables are specific to YOU - make sure to change them\n",
    "export FABRIC_BASTION_USERNAME=\"xxx\"\n",
    "export FABRIC_PROJECT_ID=\"xxx\"\n",
    "\n",
    "# these variables are specific to the filenames, locations etc. described above. \n",
    "# you may need to change them if you didn't follow the instructions above exactly.\n",
    "export FABRIC_TOKEN_LOCATION=${HOME}/work/fabric_config/id_token.json\n",
    "export FABRIC_BASTION_KEY_LOCATION=${HOME}/work/fabric_config/fabric_bastion_key\n",
    "export FABRIC_SLICE_PRIVATE_KEY_FILE=${HOME}/work/fabric_config/slice_key\n",
    "export FABRIC_SLICE_PUBLIC_KEY_FILE=${FABRIC_SLICE_PRIVATE_KEY_FILE}.pub\n",
    "\n",
    "\n",
    "# you shouldn't need to change these variables\n",
    "export FABRIC_CREDMGR_HOST=cm.fabric-testbed.net\n",
    "export FABRIC_ORCHESTRATOR_HOST=orchestrator.fabric-testbed.net\n",
    "\n",
    "\n",
    "# Now, write out the FABRIC config file\n",
    "export FABRIC_RC_FILE=${HOME}'/work/fabric_config/fabric_rc'\n",
    "\n",
    "cat <<EOF > ${FABRIC_RC_FILE}\n",
    "export FABRIC_CREDMGR_HOST=cm.fabric-testbed.net\n",
    "export FABRIC_ORCHESTRATOR_HOST=orchestrator.fabric-testbed.net\n",
    "\n",
    "export FABRIC_PROJECT_ID=${FABRIC_PROJECT_ID}\n",
    "export FABRIC_TOKEN_LOCATION=${FABRIC_TOKEN_LOCATION}\n",
    "\n",
    "export FABRIC_BASTION_HOST=bastion-1.fabric-testbed.net\n",
    "export FABRIC_BASTION_USERNAME=${FABRIC_BASTION_USERNAME}\n",
    "\n",
    "export FABRIC_BASTION_KEY_LOCATION=${FABRIC_BASTION_KEY_LOCATION}\n",
    "#export FABRIC_BASTION_KEY_PASSPHRASE=\n",
    "\n",
    "export FABRIC_SLICE_PRIVATE_KEY_FILE=${FABRIC_SLICE_PRIVATE_KEY_FILE}\n",
    "export FABRIC_SLICE_PUBLIC_KEY_FILE=${FABRIC_SLICE_PUBLIC_KEY_FILE} \n",
    "#export FABRIC_SLICE_PRIVATE_KEY_PASSPHRASE=\n",
    "\n",
    "export FABRIC_LOG_FILE=/tmp/fablib.log\n",
    "export FABRIC_LOG_LEVEL=INFO \n",
    "EOF\n",
    "\n",
    "# and an SSH config file\n",
    "export FABRIC_BASTION_SSH_CONFIG_FILE=${HOME}'/work/fabric_config/ssh_config'\n",
    "\n",
    "cat <<EOF > ${FABRIC_BASTION_SSH_CONFIG_FILE}\n",
    "UserKnownHostsFile /dev/null\n",
    "StrictHostKeyChecking no\n",
    "ServerAliveInterval 120 \n",
    "\n",
    "Host bastion-?.fabric-testbed.net\n",
    "     User ${FABRIC_BASTION_USERNAME}\n",
    "     ForwardAgent yes\n",
    "     Hostname %h\n",
    "     IdentityFile ${FABRIC_BASTION_KEY_LOCATION}\n",
    "     IdentitiesOnly yes\n",
    "\n",
    "Host * !bastion-?.fabric-testbed.net\n",
    "     ProxyJump ${FABRIC_BASTION_USERNAME}@bastion-1.fabric-testbed.net:22\n",
    "EOF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install FABRIC tools on Chameleon\n",
    "\n",
    "Now, install Python libraries needed for FABRIC:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "pip -q install --no-warn-script-location --upgrade --no-deps --force-reinstall jinja2 --user\n",
    "pip -q install --user fabrictestbed fabrictestbed-extensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify your FABRIC configuration\n",
    "\n",
    "Finally, make sure your FABRIC configuration is loaded correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['FABRIC_RC_FILE']=os.environ['HOME']+'/work/fabric_config/fabric_rc'\n",
    "os.environ['FABRIC_BASTION_SSH_CONFIG_FILE']=os.environ['HOME']+'/work/fabric_config/ssh_config'\n",
    "\n",
    "from fabrictestbed_extensions.fablib.fablib import FablibManager as fablib_manager\n",
    "fablib = fablib_manager()        \n",
    "fablib.show_config()             "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "### Adaptive video\n",
    "\n",
    "In general high-quality video requires a higher data rate than a lower-quality equivalent. Consider the following two video frames. The first shows a video encoded at 200kbps:\n",
    "\n",
    "![](https://witestlab.poly.edu/blog/content/images/2016/02/dash-200.png)\n",
    "\n",
    "Here’s the same frame at 500kbps, with noticeably better quality:\n",
    "\n",
    "![](https://witestlab.poly.edu/blog/content/images/2016/02/dash-500.png)\n",
    "\n",
    "For web services that want to share video with their users, this poses a dilemma - what quality level should they use to encode the video? If a video is low quality, it will stream without interruption even on a slow 3G cellular connection, but a user on a high speed fiber network may be unhappy with the video quality. Or, the video may be high quality, but then the slow connection would not be able to stream it without constant interruptions.\n",
    "\n",
    "Fortunately, there is a solution to this dilemma: adaptive video. Instead of delivering exactly the same video to every user, adaptive video delivers video that is matched to the individual user’s network quality.\n",
    "\n",
    "There are many different adaptive video products: Microsoft Smooth Streaming, Apple HTTP Live Streaming (HLS), Adobe HTTP Dynamic Streaming (HDS), and Dynamic Adaptive Streaming over HTTP (DASH). This experiment focuses on DASH, which is widely supported as an international standard.\n",
    "\n",
    "To prepare a video for adaptive video streaming with DASH, the video file is first encoded into different versions, each having a different rate and/or resolution. These are called *representations* or media presentations. The representations of a video all have the same content, but they differ in quality.\n",
    "\n",
    "Each of these is further subdivided in time into *segments* of equal lengths (e.g., four seconds).\n",
    "\n",
    "![](https://witestlab.poly.edu/blog/content/images/2016/02/dash-stored.png)\n",
    "\n",
    "The content server then stores all of the segments of all of the representations (as separate files). Alongside these files, the content server stores a manifest file, called the Media Presentation Description (MPD). This is an XML file that identifies the various representations, identifies the video resolution and playback rate for each, and gives the location of every segment in each representation.\n",
    "\n",
    "With these preparations complete, a user can begin to stream adaptive video from the server!\n",
    "\n",
    "Once the MPD and video files are in place, users can start requesting DASH video.\n",
    "\n",
    "First, the user requests the MPD file. It parses the MPD file, learns what representations are available, and decides what representation to request for the first segment. It then retrieves that specific file using the URL given in the MPD.\n",
    "\n",
    "The user’s device keeps a video buffer (at the application layer). As new segments are retrieved, they are placed in the buffer. As video is played back, it is removed from the buffer.\n",
    "\n",
    "Each time a client finishes retrieving a file into the buffer, it makes a new decision as to what representation to get for the next segment.\n",
    "\n",
    "For example, the client might request the following representations for the first four segments of video:\n",
    "\n",
    "![](https://witestlab.poly.edu/blog/content/images/2016/02/dash-requested.png)\n",
    "\n",
    "The cumulative set of decisions made by the client is called a decision policy. The decision policy is a set of rules that determine which representation to request, based on some kind of client state - for example, what the current download rate is, or how much video is currently stored in the buffer.\n",
    "\n",
    "The decision policy is not specified in the DASH standard. Many decision policies have been proposed by researchers, each promising to deliver better quality than the next!\n",
    "\n",
    "### DASH decision policies\n",
    "\n",
    "The obvious policy to maximize video quality alone would be to always retrive segments at the highest quality level. However, with this policy the user is likely to experience rebuffering - when playback is interrupted and the user has to wait for more video to be downloaded. This occurs when the video is being played back (and therefore, removed from the buffer) faster than it is being retrieved - i.e., the playback rate is higher than the download rate - so the buffer becomes empty. This state, which is known as buffer starvation, is obviously something we wish very much to avoid.\n",
    "\n",
    "To create a positive user experience for streaming video, therefore, requires a delicate balancing act.\n",
    "\n",
    "-   On the one hand, increasing the video playback rate too much (so that it is higher than the download rate) causes the undesired rebuffers.\n",
    "-   On the other hand, decreasing the video playback rate also decreases the user-perceived video quality.\n",
    "\n",
    "Performing rate selection to balance rebuffer avoidance and quality optimization is an ongoing tradeoff. Different DASH policies may make different decisions about how to balance that tradeoff. Different DASH policies may also decide to use different pieces of information for decision making. For example:\n",
    "\n",
    "-   A decision policy may decide to focus on download rate in its decision making - select the quality level for the next video segment according to the download rate from the previous segment(s).\n",
    "-   Or, a decision policy may focus on buffer occupancy (how much video is already downloaded into the buffer, waiting to be played back?) If there is already a lot of video in the buffer, the decision policy can afford to be aggressive in its quality selection, since it has a cushion to protect it from rebuffering. On the other hand, if there is not much video in the buffer, the decision policy should be careful not to select a quality level that is too optimistic, since it is at high risk of rebuffering.\n",
    "\n",
    "### Specific policies in this implementation\n",
    "\n",
    "In this experiment, we will use an updated version of the DASH implementation developed for the following paper:\n",
    "\n",
    "> P. Juluri, V. Tamarapalli and D. Medhi, “SARA: Segment aware rate adaptation algorithm for dynamic adaptive streaming over HTTP,” 2015 IEEE International Conference on Communication Workshop (ICCW), 2015, pp. 1765-1770, doi: 10.1109/ICCW.2015.7247436.\n",
    "\n",
    "which you can browse on [Github](https://github.com/teaching-on-testbeds/AStream). It includes three DASH decision policies:\n",
    "\n",
    "The “basic” policy is a rate-based policy that tries to keep the video rate at or below the current network data rate. You can see [the “basic” implementation here](https://github.com/teaching-on-testbeds/AStream/blob/master/dist/client/adaptation/basic_dash2.py).\n",
    "\n",
    "The buffer-based rate adaptation (“netflix”) algorithm uses the estimated network data rate only during the initial startup phase. Otherwise, it makes quality decisions based on the buffer occupancy, i.e. tring to avoid an empty buffer which would cause the video to freeze. It is based on the algorithm described in the following paper:\n",
    "\n",
    "> Te-Yuan Huang, Ramesh Johari, Nick McKeown, Matthew Trunnell, and Mark Watson. 2014. A buffer-based approach to rate adaptation: evidence from a large video streaming service. In Proceedings of the 2014 ACM conference on SIGCOMM (SIGCOMM ’14). Association for Computing Machinery, New York, NY, USA, 187–198. DOI:https://doi.org/10.1145/2619239.2626296\n",
    "\n",
    "You can see [the “Netflix” implementation here](https://github.com/teaching-on-testbeds/AStream/blob/master/dist/client/adaptation/netflix_dash.py).\n",
    "\n",
    "Finally, the segment-aware rate adaptation (“SARA”) algorithm uses the actual size of the segment and data rate of the network to estimate the time it would take to download the next segment. Then, given the current buffer occupancy, it selects the best possible video quality while avoiding buffer starvation. It is described in\n",
    "\n",
    "> P. Juluri, V. Tamarapalli and D. Medhi, “SARA: Segment aware rate adaptation algorithm for dynamic adaptive streaming over HTTP,” 2015 IEEE International Conference on Communication Workshop (ICCW), 2015, pp. 1765-1770, doi: 10.1109/ICCW.2015.7247436.\n",
    "\n",
    "You can see [the “SARA” implementation here](https://github.com/teaching-on-testbeds/AStream/blob/master/dist/client/adaptation/weighted_dash.py)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reserve resources on FABRIC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this experiment, we will use three virtual machines, connected in a linear topology: a client, a router, and a server. In this section, you will reserve and configure these resources on FABRIC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load your FABRIC configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following instructions assume you have already configured your JupyterHub environment in a previous session, including creating the `fabric_rc` and `ssh_config` files. If you haven’t, you should do that first - it’s a prerequisite for this experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load your FABRIC configuration options, then check to make sure the configuration looks correct:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['FABRIC_RC_FILE']=os.environ['HOME']+'/work/fabric_config/fabric_rc'\n",
    "os.environ['FABRIC_BASTION_SSH_CONFIG_FILE']=os.environ['HOME']+'/work/fabric_config/ssh_config'\n",
    "\n",
    "from fabrictestbed_extensions.fablib.fablib import FablibManager as fablib_manager\n",
    "fablib = fablib_manager()                     \n",
    "fablib.show_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure the private key file you will use to access resources has the appropriate permissions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['FABRIC_SLICE_PRIVATE_KEY_FILE'] = fablib.get_default_slice_private_key_file()\n",
    "os.environ['FABRIC_BASTION_PRIVATE_KEY_FILE'] = fablib.get_bastion_key_filename()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "chmod 0600 \"$FABRIC_SLICE_PRIVATE_KEY_FILE\"\n",
    "chmod 400 \"$FABRIC_BASTION_PRIVATE_KEY_FILE\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare a slice for this experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If everything looks good, let’s set up a slice! We’ll name our slice for this experiment using a combination of your username and the name `adaptive_video`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SLICENAME=fablib.get_bastion_username() + \"_adaptive_video\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you already have the resources for this experiment (for example: you ran this part of the notebook previously, and are now returning to pick off where you left off), you don’t need to reserve resources again. If the following cell tells you that you already have resources, you can just skip ahead to the part of the experiment where you left off last."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    fablib.get_slice(SLICENAME)\n",
    "    print(\"You already have a slice named %s.\\nYou should skip the 'Reserve resources in your slice' section.\" % SLICENAME)\n",
    "    slice = fablib.get_slice(name=SLICENAME)\n",
    "except:\n",
    "    print(\"You don't have any active slice named %s.\\nKeep going to set one up!\" % SLICENAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reserve resources in your slice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you don’t already have a slice with the resources for this experiment, you’ll reserve one now! First, you’ll select a FABRIC site on which to run your experiment.\n",
    "\n",
    "The following cell will select a random FABRIC site. Check the output of this cell and make sure the selected site has sufficient resources - for this experiment, your selected site should have at least:\n",
    "\n",
    "-   3 cores (1 per VM)\n",
    "-   12 GB RAM (4 GB per VM)\n",
    "-   30 GB disk space (10 GB per VM)\n",
    "\n",
    "Re-run the cell to select a new random site until you find one with available resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "SITE = random.choice(fablib.get_site_names())\n",
    "print(f\"{fablib.show_site(SITE)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have selected a site, you can reserve resources at that site. The following cell will set up your resource request and then submit it to FABRIC.\n",
    "\n",
    "The output of the cell will update automatically as your slice status changes. It may take a while (5-10 minutes) before this process is complete and the “Slice State” changes to “StableOK”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slice = fablib.new_slice(name=SLICENAME)\n",
    "\n",
    "nodes = {'romeo': None, 'juliet': None, 'router': None}\n",
    "for key,val in nodes.items():\n",
    "    nodes[key] = slice.add_node(name=key,  site=SITE, cores=1, ram=4, disk=10, image='default_ubuntu_20')\n",
    "\n",
    "iface_net_r = [\n",
    "    nodes['romeo'].add_component(model=\"NIC_Basic\", name=\"if_romeo\").get_interfaces()[0],\n",
    "    nodes['router'].add_component(model=\"NIC_Basic\", name=\"if_router_r\").get_interfaces()[0]\n",
    "]\n",
    "slice.add_l2network(name='net_r', type='L2Bridge', interfaces=iface_net_r)\n",
    "\n",
    "iface_net_j = [\n",
    "    nodes['juliet'].add_component(model=\"NIC_Basic\", name=\"if_juliet\").get_interfaces()[0],\n",
    "    nodes['router'].add_component(model=\"NIC_Basic\", name=\"if_router_j\").get_interfaces()[0]\n",
    "]\n",
    "slice.add_l2network(name='net_j', type='L2Bridge', interfaces=iface_net_j)\n",
    "\n",
    "slice.submit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When it is done, verify that the slice status is “StableOK”:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{slice}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure your slice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start our experiment, we need to configure the resources and the network on this slice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’ll install some software on the end hosts. This cell may take another 10 minutes, and no output will appear until it is finished running:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slice.get_node(\"romeo\").execute(\"sudo apt update; sudo apt -y install net-tools iperf3 moreutils\")\n",
    "slice.get_node(\"juliet\").execute(\"sudo apt update; sudo apt -y install net-tools iperf3 moreutils\")\n",
    "slice.get_node(\"router\").execute(\"sudo apt update; sudo apt -y install net-tools\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will set up networking."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell will make sure that the FABRIC nodes can reach targets on the Internet (e.g. to retrieve files or software), even if the FABRIC nodes connect to the Internet through IPv6 and the targetes on the Internet are IPv4 only, by using [nat64](https://nat64.net/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for node in [\"romeo\", \"juliet\", \"router\"]:\n",
    "    slice.get_node(node).execute('sudo sed -i \"1s/^/nameserver 2a01:4f9:c010:3f02::1\\\\n/\" /etc/resolv.conf')\n",
    "    slice.get_node(node).execute('echo \"127.0.0.1 $(hostname -s)\" | sudo tee -a /etc/hosts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure an IP address on every experiment interface\n",
    "from ipaddress import IPv4Address, IPv4Network\n",
    "slice.get_interface(\"romeo-if_romeo-p1\").ip_addr_add(IPv4Address('192.168.0.2'), IPv4Network('192.168.0.0/24'))\n",
    "slice.get_interface(\"router-if_router_r-p1\").ip_addr_add(IPv4Address('192.168.0.1'), IPv4Network('192.168.0.0/24'))\n",
    "slice.get_interface(\"router-if_router_j-p1\").ip_addr_add(IPv4Address('192.168.1.1'), IPv4Network('192.168.1.0/24'))\n",
    "slice.get_interface(\"juliet-if_juliet-p1\").ip_addr_add(IPv4Address('192.168.1.2'), IPv4Network('192.168.1.0/24'))\n",
    "\n",
    "# bring all the interfaces up\n",
    "slice.get_interface(\"romeo-if_romeo-p1\").ip_link_up()\n",
    "slice.get_interface(\"router-if_router_r-p1\").ip_link_up()\n",
    "slice.get_interface(\"router-if_router_j-p1\").ip_link_up()\n",
    "slice.get_interface(\"juliet-if_juliet-p1\").ip_link_up()\n",
    "\n",
    "# enable IP forwarding on router\n",
    "slice.get_node(\"router\").execute(\"sudo sysctl -w net.ipv4.ip_forward=1\")\n",
    "\n",
    "# add a route on each host to reach the other host via router\n",
    "slice.get_node(\"romeo\").ip_route_add(IPv4Network('192.168.1.0/24'), IPv4Address('192.168.0.1'))\n",
    "slice.get_node(\"juliet\").ip_route_add(IPv4Network('192.168.0.0/24'), IPv4Address('192.168.1.1'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To validate this setup, we will run a `ping` test from “romeo” to “juliet”. The following cell *must* return `True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slice.get_node(\"romeo\").ping_test('192.168.0.2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfer video application to the client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this experiment, we will work with a DASH video client-let’s transfer it to the “romeo” node now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git submodule update --init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slice.get_node(\"romeo\").upload_directory('../AStream','~/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Later, if you want to modify the source code of the DASH video client (e.g. modify [the basic policy](../AStream/dist/client/adaptation/basic_dash2.py)), you can make your changes loacally and then repeat this section to transfer the modified code to the “romeo” node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get login details for your slice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can get the SSH command to log in to each host in the slice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for node in slice.get_nodes():\n",
    "    print(f\"{node.get_name()}: {node.get_ssh_command()} -F {os.environ['FABRIC_BASTION_SSH_CONFIG_FILE']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To open an SSH session on any host, use File \\> New \\> Terminal. Copy the SSH command from the output of the cell above to this terminal session, and use it to log in to the remote host."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the adaptive video experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we’re going to install software and set up the materials we need specifically to transfer adaptive video across this network! You will do this by opening SSH sessions to each of the hosts in the topology, and running commands to set them up as needed.\n",
    "\n",
    "Make sure you have the SSH commands ready for each of the hosts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the server\n",
    "\n",
    "First, we will set up the “juliet” host as an adaptive video server. Open an SSH session on “juliet”, and run the commands in this section there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the server, we will set up an HTTP server which will serve the video files to the client.\n",
    "\n",
    "First, install the Apache HTTP server:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` bash\n",
    "sudo apt update  \n",
    "sudo apt install -y apache2  \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, download the video segments and put them in the web server directory. This step will take some time - while it is running, you can open another tab and move on to configuration of the other hosts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` bash\n",
    "wget https://nyu.box.com/shared/static/d6btpwf5lqmkqh53b52ynhmfthh2qtby.tgz -O media.tgz\n",
    "sudo tar -v -xzf media.tgz -C /var/www/html/\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The web server directory now contains 4-second segments of the “open” video clip [Big Buck Bunny](https://peach.blender.org/about/), encoded at different quality levels. The Big Buck Bunny DASH dataset is from:\n",
    "\n",
    "> Stefan Lederer, Christopher Müller, and Christian Timmerer. 2012. Dynamic adaptive streaming over HTTP dataset. In Proceedings of the 3rd Multimedia Systems Conference (MMSys ’12). Association for Computing Machinery, New York, NY, USA, 89–94. DOI:https://doi.org/10.1145/2155555.2155570"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the router\n",
    "\n",
    "Next, we will set up the router. Open an SSH session on “router”, and run the commands in this section there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the router, we will *emulate* different network conditions, to see how each DASH policy performs.\n",
    "\n",
    "We will experiment with both a constant data rate, and a variable data rate like that experienced by a mobile user. For the mobile user, we’ll use some network traces collected in the New York City metro area. With these traces, the data rate experienced by the DASH client in our experiment will mimic the experience of traveling around NYC on bus, subway, and ferry.\n",
    "\n",
    "The NYC traces are shared from the following paper:\n",
    "\n",
    "> Lifan Mei, Runchen Hu, Houwei Cao, Yong Liu, Zifa Han, Feng Li & Jin Li. (2019, March). Realtime Mobile Bandwidth Prediction using LSTM Neural Networks. In International Conference on Passive and Active Network Measurement. Springer.\n",
    "\n",
    "To download the traces, on the “router” node run:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` bash\n",
    "git clone https://github.com/NYU-METS/Main nyc-traces\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To extract the trace files from their compressed archive, we will need to install an appropriate utility:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` bash\n",
    "sudo apt update\n",
    "sudo apt install -y unrar-free\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` bash\n",
    "unrar nyc-traces/Dataset/Dataset_1.rar\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also download a couple of utility scripts to help us set a constant data rate or vary the data rate on the network. On the “router” node, run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` bash\n",
    "wget https://raw.githubusercontent.com/teaching-on-testbeds/adaptive-video/main/rate-vary.sh -O ~/rate-vary.sh\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` bash\n",
    "wget https://raw.githubusercontent.com/teaching-on-testbeds/adaptive-video/main/rate-set.sh -O ~/rate-set.sh\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the client\n",
    "\n",
    "Finally, we need to prepare the “romeo” host as a video client. Open an SSH session on “romeo”, and run the commands in this section there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We must install Python3 to run the DASH video client, and we will also install the video encoding utility `ffmpeg` so that we can reconstruct the video later:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` bash\n",
    "sudo apt update\n",
    "sudo apt install -y python3 ffmpeg\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to run our experiments! We will run four experiments: one with a constant bit rate, one with a constant bit rate and an interruption in middle,one with where comparing the policies (with constant bit rate and an interruption in middle) and one with a varying bit rate using the NYC traces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute constant bit rate experiment\n",
    "\n",
    "For this section, you will need an SSH session on the “router” node and one on the “romeo” node.\n",
    "\n",
    "On the “router”, set a constant bit rate of 1000 Kbits/second with\n",
    "\n",
    "``` bash\n",
    "bash rate-set.sh 1000Kbit\n",
    "```\n",
    "\n",
    "(The first time you run it, you may see an error referencing a problem deleting a `qdisc`, but you can safely ignore this error.)\n",
    "\n",
    "Note: you can specify a data rate in Kbits/second using `Kbit` or in Mbits/second using `Mbit`.\n",
    "\n",
    "Then, on the client (“romeo”), start the DASH player with the “basic” adaptation policy:\n",
    "\n",
    "``` bash\n",
    "python3 ~/AStream/dist/client/dash_client.py -m http://192.168.1.2/media/BigBuckBunny/4sec/BigBuckBunny_4s.mpd -p 'basic' -d\n",
    "```\n",
    "\n",
    "(Note: you can alternatively try `netflix` or `sara` as the DASH policy.)\n",
    "\n",
    "Leave this running for a while. Then, you can interrupt the DASH client with Ctrl+C.\n",
    "\n",
    "To understand the performance of the DASH policy, we can look at the logs produced by the client. These will be located inside a directory named `ASTREAM_LOGS` in your home directory on the “romeo” node. Use\n",
    "\n",
    "``` bash\n",
    "ls ~/ASTREAM_LOGS\n",
    "```\n",
    "\n",
    "to find these.\n",
    "\n",
    "In the data analysis section, we will use these logs - specifically the one that begins with `DASH_BUFFER_LOG_` - to understand the video adaptation policy that was applied in this experiment. We will copy the file associated with *this* experiment to `~/ASTREAM_LOGS/DASH_BUFFER_LOG_last.csv` with\n",
    "\n",
    "``` bash\n",
    "cp $(ls -t1  ~/ASTREAM_LOGS/DASH_BUFFER_LOG_*  | head -n 1 ) ~/ASTREAM_LOGS/DASH_BUFFER_LOG-last.csv\n",
    "```\n",
    "\n",
    "Also reconstruct the video that was delivered to the client. Use\n",
    "\n",
    "``` bash\n",
    "suffix=$(ls -lt | grep \"TEMP_\" | head -n 1 | cut -f2 -d\"_\")\n",
    "cd ~/TEMP_$suffix\n",
    "rm -f ~/BigBuckBunny.mp4 # if it exists\n",
    "cat BigBuckBunny_4s_init.mp4 $(ls -vx BigBuckBunny_*.m4s) > BigBuckBunny_tmp.mp4\n",
    "ffmpeg -i  BigBuckBunny_tmp.mp4 -c copy ~/BigBuckBunny.mp4\n",
    "```\n",
    "\n",
    "to combine the video segments into a `BigBuckBunny.mp4` file in your home directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data analysis\n",
    "\n",
    "After each experiment run (with different variations in experiment conditions!) we can see the video that was delivered to the client, and also see how the video client made its decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, run this cell to make sure your FABRIC configuration and slice configuration is loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['FABRIC_RC_FILE']=os.environ['HOME']+'/work/fabric_config/fabric_rc'\n",
    "os.environ['FABRIC_BASTION_SSH_CONFIG_FILE']=os.environ['HOME']+'/work/fabric_config/ssh_config'\n",
    "\n",
    "from fabrictestbed_extensions.fablib.fablib import FablibManager as fablib_manager\n",
    "fablib = fablib_manager()                     \n",
    "\n",
    "SLICENAME=fablib.get_bastion_username() + \"_adaptive_video\"\n",
    "slice = fablib.get_slice(name=SLICENAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, the following cell will retrieve the reconstructed video file so that you can play it back inside this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slice.get_node(\"romeo\").download_file(\"/home/fabric/work/BigBuckBunny.mp4\", \"/home/ubuntu/BigBuckBunny.mp4\")\n",
    "from IPython.display import Video\n",
    "Video(\"/home/fabric/work/BigBuckBunny.mp4\", embed=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, you can use the log files to find out how the video client made its decisions. In the following cell, fill in the `DASH_BUFFER_LOG` log file name associated with the instance of *your* experiment that you want to analyze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DASH_BUFFER_LOG=\"DASH_BUFFER_LOG-last.csv\"\n",
    "slice.get_node(\"romeo\").download_file(\"/home/fabric/work/DASH_BUFFER_LOG.csv\", \"/home/ubuntu/ASTREAM_LOGS/\" + DASH_BUFFER_LOG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and use the following cell to create a visualization. In the following plot, the line shows the bit rate of each segment as it is played back over time, and the colored background indicates whether the client is playing video (light cyan) or buffering (light pink)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "c = {'INITIAL_BUFFERING': 'violet', 'PLAY': 'lightcyan', 'BUFFERING': 'lightpink'}\n",
    "\n",
    "dash = pd.read_csv(\"/home/fabric/work/DASH_BUFFER_LOG.csv\")\n",
    "dash = dash.loc[dash.CurrentPlaybackState.isin(c.keys() )]\n",
    "states = pd.DataFrame({'startState': dash.CurrentPlaybackState[0:-2].values, 'startTime': dash.EpochTime[0:-2].values,\n",
    "                        'endState':  dash.CurrentPlaybackState[1:-1].values, 'endTime':   dash.EpochTime[1:-1].values})\n",
    "\n",
    "\n",
    "for index, s in states.iterrows():\n",
    "  plt.axvspan(s['startTime'], s['endTime'],  color=c[s['startState']], alpha=1) \n",
    "\n",
    "plt.plot(dash[dash.Action!=\"Writing\"].EpochTime, dash[dash.Action!=\"Writing\"].Bitrate, 'kx:')\n",
    "plt.title(\"Video rate (bps)\");\n",
    "plt.xlabel(\"Time (s)\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also visualize the buffer occupancy over time. In the following plot, the line shows the number of segments in the buffer over time, and the colored background indicates whether the client is playing video (light cyan) or buffering (light pink). When the buffer occupancy goes to zero, the will have to stop playing in order to retrieve more data into the buffer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "c = {'INITIAL_BUFFERING': 'violet', 'PLAY': 'lightcyan', 'BUFFERING': 'lightpink'}\n",
    "\n",
    "dash = pd.read_csv(\"/home/fabric/work/DASH_BUFFER_LOG.csv\")\n",
    "dash = dash.loc[dash.CurrentPlaybackState.isin(c.keys() )]\n",
    "states = pd.DataFrame({'startState': dash.CurrentPlaybackState[0:-2].values, 'startTime': dash.EpochTime[0:-2].values,\n",
    "                        'endState':  dash.CurrentPlaybackState[1:-1].values, 'endTime':   dash.EpochTime[1:-1].values})\n",
    "\n",
    "\n",
    "for index, s in states.iterrows():\n",
    "  plt.axvspan(s['startTime'], s['endTime'],  color=c[s['startState']], alpha=1) \n",
    "\n",
    "plt.plot(dash[dash.Action!=\"Writing\"].EpochTime, dash[dash.Action!=\"Writing\"].CurrentBufferSize, 'kx:')\n",
    "plt.title(\"Buffer(segments)\");\n",
    "plt.xlabel(\"Time (s)\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute constant bit rate experiment with interruption\n",
    "\n",
    "For this section, you will need an SSH session on the “router” node and one on the “romeo” node.\n",
    "\n",
    "In the experiment with constant bit rate, you may not have experienced any rebuffering.\n",
    "\n",
    "To see how the video client works when there is a temporary interruption in the network, try repeating this experiment, but during the video session, reduce the network data rate to a very low value in middle of the session.\n",
    "\n",
    "On the “router”, set a constant bit rate of 1000 Kbits/second with\n",
    "\n",
    "``` bash\n",
    "bash rate-set.sh 1000Kbit\n",
    "```\n",
    "\n",
    "Then, on the client (“romeo”), start the DASH player with the “basic” adaptation policy:\n",
    "\n",
    "``` bash\n",
    "python3 ~/AStream/dist/client/dash_client.py -m http://192.168.1.2/media/BigBuckBunny/4sec/BigBuckBunny_4s.mpd -p 'basic' -d\n",
    "```\n",
    "\n",
    "Leave this running for a while. Then, on the “router”, reduce the network data rate to 50 Kbits/second:\n",
    "\n",
    "    bash rate-set.sh 50Kbit\n",
    "\n",
    "After some time has elapsed, restore the original data rate.\n",
    "\n",
    "``` bash\n",
    "bash rate-set.sh 1000Kbit\n",
    "```\n",
    "\n",
    "Then, after a little while longer, stop the video client on “romeo” with Ctrl+C.\n",
    "\n",
    "As before, the logs produced by the client will be located inside a directory named `ASTREAM_LOGS` in your home directory on the “romeo” node. Use\n",
    "\n",
    "``` bash\n",
    "ls ~/ASTREAM_LOGS\n",
    "```\n",
    "\n",
    "to find these. We will copy the file associated with *this* experiment to `~/ASTREAM_LOGS/DASH_BUFFER_LOG_last.csv` with\n",
    "\n",
    "``` bash\n",
    "cp $(ls -t1  ~/ASTREAM_LOGS/DASH_BUFFER_LOG_*  | head -n 1 ) ~/ASTREAM_LOGS/DASH_BUFFER_LOG-last.csv\n",
    "```\n",
    "\n",
    "Also reconstruct the video that was delivered to the client. Use\n",
    "\n",
    "``` bash\n",
    "suffix=$(ls -lt | grep \"TEMP_\" | head -n 1 | cut -f2 -d\"_\")\n",
    "cd ~/TEMP_$suffix\n",
    "rm -f ~/BigBuckBunny.mp4 # if it exists\n",
    "cat BigBuckBunny_4s_init.mp4 $(ls -vx BigBuckBunny_*.m4s) > BigBuckBunny_tmp.mp4\n",
    "ffmpeg -i  BigBuckBunny_tmp.mp4 -c copy ~/BigBuckBunny.mp4\n",
    "```\n",
    "\n",
    "to combine the video segments into a `BigBuckBunny.mp4` file in your home directory.\n",
    "\n",
    "Then, you can repeat the data analysis steps as before.\n",
    "\n",
    "## Compare adaptive video policies\n",
    "\n",
    "For this section, you will need an SSH session on the “router” node and one on the “romeo” node.\n",
    "\n",
    "As in the previous experiment, in this experiment we will use a constant bit rate, with a brief interruption. However, we will compare the way two video adaptation policies react to this interruption - we’ll compare a *rate based policy* (`basic`) and a *buffer based policy* (`netflix`) under identical settings (similar “high” network rate, “low” data rate, similar duration of the “high” data rate before the interruption, and and similar duration of the “interruption”).\n",
    "\n",
    "#### Rate-based vs. buffer-based policies\n",
    "\n",
    "The basic rate adaptation policy (`basic`) chooses a video rate based on the observed download rate. It keeps track of the average download time for recent segments, and calculates a running average.\n",
    "\n",
    "If the download rate exceeds the current video rate by some threshold, it may increase the video rate. If the download rate is lower than the current video rate, it decreases the video rate to ensure smooth playback.\n",
    "\n",
    "You can see the source code for the `basic` policy here: [basic_dash2.py](../AStream/dist/client/adaptation/basic_dash2.py)\n",
    "\n",
    "The buffer-based policy (`netflix`) adapts the video rate based on the current buffer occupancy, rather than the current download rate. When there are many segments already buffered, it can increase the video rate; if the buffer occupancy is low and the client is at risk of rebuffering, it must decrease the video rate.\n",
    "\n",
    "You can see the source code for the `netflix` policy here: [netflix_dash.py](../AStream/dist/client/adaptation/netflix_dash.py). This policy is based on the paper:\n",
    "\n",
    "> Te-Yuan Huang, Ramesh Johari, and Nick McKeown. 2013. Downton abbey without the hiccups: buffer-based rate adaptation for HTTP video streaming. In Proceedings of the 2013 ACM SIGCOMM workshop on Future human-centric multimedia networking (FhMN ’13). Association for Computing Machinery, New York, NY, USA, 9–14. https://doi.org/10.1145/2491172.2491179\n",
    "\n",
    "The policy defines two buffer occupancy thresholds: reservoir (defaults to 10%) and cushion (defaults to 90%). If the buffer occupancy is below the reservoir threshold, it selects the minimum video rate to fill the buffer quickly. If the buffer is within the reservoir and cushion range, it selects a video rate using a rate map function that maps buffer occupancy to video rate according to some increasing function. If the buffer occupancy exceeds the cushion threshold, it selects the maximum video rate.\n",
    "\n",
    "A brief explanation of the key variables in this policy follows:\n",
    "\n",
    "-   **Reservoir**: Imagine you have a water tank that supplies water to your house. The “reservoir” in this context is like the water level at the bottom of the tank. It’s the minimum amount of water that needs to be there at all times to ensure a steady and uninterrupted water supply. If the reservoir is too low, you might experience water interruptions. Similarly, in video streaming, the reservoir is the minimum amount of video content that must be stored in the buffer to ensure a smooth playback experience. It’s like having a small reserve of video data to prevent any pauses or disruptions if there are temporary changes in network conditions.\n",
    "-   **Cushion**: Think of a cushion on a couch. It’s a soft layer that provides comfort and support. The “cushion” in this context is like an extra layer of video content stored in the buffer above the reservoir level. It’s there to give extra protection against sudden changes. Just as a cushion absorbs some impact, this cushion of video content absorbs any fluctuations in network performance. In video streaming, the cushion is an additional amount of video data stored in the buffer beyond the reservoir level. It’s like having a padding of video content that helps maintain a consistent and uninterrupted playback experience, even when there are brief variations in network speed.\n",
    "\n",
    "Let’s get started!\n",
    "\n",
    "### Execute the experiment for the rate based policy\n",
    "\n",
    "On the “router”, set a constant bit rate of 5000 Kbits/second with\n",
    "\n",
    "``` bash\n",
    "bash rate-set.sh 5000Kbit\n",
    "```\n",
    "\n",
    "Then, on the client (“romeo”), start the DASH player with the “basic” adaptation policy and start the timer along with:\n",
    "\n",
    "``` bash\n",
    "python3 ~/AStream/dist/client/dash_client.py -m http://192.168.1.2/media/BigBuckBunny/4sec/BigBuckBunny_4s.mpd -p 'basic' -d\n",
    "```\n",
    "\n",
    "Let the DASH player run for 100 seconds. After this duration, on the “router” node, reduce the network data rate to 350 Kbits/second:\n",
    "\n",
    "``` bash\n",
    "bash rate-set.sh 350Kbit\n",
    "```\n",
    "\n",
    "After an additional 75 seconds (175 seconds total runtime so far), restore the original data rate.\n",
    "\n",
    "``` bash\n",
    "bash rate-set.sh 5000Kbit\n",
    "```\n",
    "\n",
    "Then, after 300 second, stop the video client on “romeo” with Ctrl+C.\n",
    "\n",
    "As before, the logs produced by the client will be located inside a directory named `ASTREAM_LOGS` in your home directory on the “romeo” node. Use\n",
    "\n",
    "``` bash\n",
    "ls ~/ASTREAM_LOGS\n",
    "```\n",
    "\n",
    "to find these. We will copy the file associated with *this* experiment to `~/ASTREAM_LOGS/DASH_BUFFER_LOG_last.csv` with\n",
    "\n",
    "``` bash\n",
    "cp $(ls -t1  ~/ASTREAM_LOGS/DASH_BUFFER_LOG_*  | head -n 1 ) ~/ASTREAM_LOGS/DASH_BUFFER_LOG-last.csv\n",
    "```\n",
    "\n",
    "Also reconstruct the video that was delivered to the client. Use\n",
    "\n",
    "``` bash\n",
    "suffix=$(ls -lt | grep \"TEMP_\" | head -n 1 | cut -f2 -d\"_\")\n",
    "cd ~/TEMP_$suffix\n",
    "rm -f ~/BigBuckBunny.mp4 # if it exists\n",
    "cat BigBuckBunny_4s_init.mp4 $(ls -vx BigBuckBunny_*.m4s) > BigBuckBunny_tmp.mp4\n",
    "ffmpeg -i  BigBuckBunny_tmp.mp4 -c copy ~/BigBuckBunny.mp4\n",
    "```\n",
    "\n",
    "to combine the video segments into a `BigBuckBunny.mp4` file in your home directory.\n",
    "\n",
    "Repeat the data analysis steps as before. Save the figures and the reconstructed video for the rate based policy, so that after completing the experiment in the next section with the buffer based policy, you can compare their behavior.\n",
    "\n",
    "### Execute the experiment for the buffer based policy\n",
    "\n",
    "On the “router”, set a constant bit rate of 5000 Kbits/second with\n",
    "\n",
    "``` bash\n",
    "bash rate-set.sh 5000Kbit\n",
    "```\n",
    "\n",
    "Then, on the client (“romeo”), start the DASH player with the “netflix” adaptation policy and start the timer along with:\n",
    "\n",
    "``` bash\n",
    "python3 ~/AStream/dist/client/dash_client.py -m http://192.168.1.2/media/BigBuckBunny/4sec/BigBuckBunny_4s.mpd -p 'netflix' -d\n",
    "```\n",
    "\n",
    "Let the DASH player run for 100 seconds. After this duration, on the “router” node, reduce the network data rate to 350 Kbits/second:\n",
    "\n",
    "``` bash\n",
    "bash rate-set.sh 350Kbit\n",
    "```\n",
    "\n",
    "After an additional 75 seconds (175 seconds total runtime so far), restore the original data rate.\n",
    "\n",
    "``` bash\n",
    "bash rate-set.sh 5000Kbit\n",
    "```\n",
    "\n",
    "Then, after 300 second, stop the video client on “romeo” with Ctrl+C.\n",
    "\n",
    "As before, the logs produced by the client will be located inside a directory named `ASTREAM_LOGS` in your home directory on the “romeo” node. Use\n",
    "\n",
    "``` bash\n",
    "ls ~/ASTREAM_LOGS\n",
    "```\n",
    "\n",
    "to find these. We will copy the file associated with *this* experiment to `~/ASTREAM_LOGS/DASH_BUFFER_LOG_last.csv` with\n",
    "\n",
    "``` bash\n",
    "cp $(ls -t1  ~/ASTREAM_LOGS/DASH_BUFFER_LOG_*  | head -n 1 ) ~/ASTREAM_LOGS/DASH_BUFFER_LOG-last.csv\n",
    "```\n",
    "\n",
    "Also reconstruct the video that was delivered to the client. Use\n",
    "\n",
    "``` bash\n",
    "suffix=$(ls -lt | grep \"TEMP_\" | head -n 1 | cut -f2 -d\"_\")\n",
    "cd ~/TEMP_$suffix\n",
    "rm -f ~/BigBuckBunny.mp4 # if it exists\n",
    "cat BigBuckBunny_4s_init.mp4 $(ls -vx BigBuckBunny_*.m4s) > BigBuckBunny_tmp.mp4\n",
    "ffmpeg -i  BigBuckBunny_tmp.mp4 -c copy ~/BigBuckBunny.mp4\n",
    "```\n",
    "\n",
    "to combine the video segments into a `BigBuckBunny.mp4` file in your home directory.\n",
    "\n",
    "Repeat the data analysis steps as before. Save the figures and the reconstructed video for the buffer based policy, for comparison with the rate based policy in the previous section.\n",
    "\n",
    "### Discussions\n",
    "\n",
    "After analyzing the video rate vs. time and buffer vs. time graphs for both the (`basic`) and (`netflix`) adaptation policies, along with the actual video playback, several key observations can be made.\n",
    "\n",
    "Both policies initially start with increasing video rates as the buffer needs to be filled quickly due to being empty. However, in the rate-based policy, the video rate increases more rapidly compared to the buffer-based policy. In the rate-based policy, once the video rate reaches its maximum value, the video runs smoothly without interruption. On the other hand, in the buffer-based policy, the video rate increases, but small interruptions (not exactly buffering) occur because the video rate has not yet reached its maximum.\n",
    "\n",
    "During the interruption phase, the differences in behavior between the two policies become more apparent. In the rate-based (`basic`) policy, there is a noticeable decrease in the video rate and buffering events. This is attributed to the policy’s focus on adjusting the video rate based on the observed download rate. As the data rate decreases, the policy responds by reducing the buffer occupancy, resulting in buffering. In contrast, the buffer-based (`netflix`) policy experiences no buffering events during interruptions. This is a direct consequence of the policy’s adaptation strategy, which relies on the current buffer occupancy rather than the download rate. The buffer-based policy ensures sufficient buffer to accommodate interruptions, leading to smooth and uninterrupted playback.\n",
    "\n",
    "The buffer-based policy (`netflix`) maintains a consistently higher buffer level throughout the experiment, even during interruptions, where it decreases but not significantly. This is evident from the buffer occupancy graph, which generally remains above the buffer threshold level “reservoir”.Conversely, the rate-based policy (`basic`) exhibits fluctuations in buffer occupancy. It begins with a lower buffer level, and during interruptions, the buffer depletes rapidly, resulting in buffering events and drops in video rate.\n",
    "\n",
    "**Video length**: Both policies were executed for a duration of 300 seconds, yet a notable difference exists in the effective video length delivered to the client. In the buffer-based policy, the delivered video length is closer to the original full video length (approximately 594 seconds), indicating more stable and continuous playback.Conversely, in the rate-based policy, the delivered video length slightly exceeds 300 seconds, highlighting that buffering events and reduced buffer size during interruptions led to incomplete video playback.\n",
    "\n",
    "## Execute experiment with varying bit rate (mobile user)\n",
    "\n",
    "Finally, you can try to experience adaptive video as a mobile user!\n",
    "\n",
    "Repeat the experiment, but instead of setting a constant data rate on the router, you can let it play back a “trace” file with e.g. \n",
    "\n",
    "``` bash\n",
    "bash rate-vary.sh ~/Dataset_1/Dataset/Ferry/Ferry5.csv 0.1\n",
    "```\n",
    "\n",
    "where the first argument is the path to a trace file, and the second argument is a scaling factor greater than 0 but less than 1. (The smaller the scaling factor, the lower the network quality while still preserving the trace dynamics.)\n",
    "\n",
    "The following figure shows the “dynamics” (throughput in Mbps against time) for each of the traces:\n",
    "\n",
    "![](https://witestlab.poly.edu/blog/content/images/2022/04/nyc-traces.png)\n",
    "\n",
    "For some traces, the throughput is always more than enough to steam the video at the highest quality level. For the traces where the throughput is *not* sufficient to stream continuously at the highest quality level, a good decision policy should still be able to smooth over the variation in network quality and deliver high quality video without rebuffering.\n",
    "\n",
    "While playing back a trace on the “router”, on the client (“romeo”), start the DASH player with the “basic” adaptation policy:\n",
    "\n",
    "``` bash\n",
    "python3 ~/AStream/dist/client/dash_client.py -m http://192.168.1.2/media/BigBuckBunny/4sec/BigBuckBunny_4s.mpd -p 'basic' -d\n",
    "```\n",
    "\n",
    "Leave this running for a while. Then, stop the video client on “romeo” with Ctrl+C.\n",
    "\n",
    "As before, the logs produced by the client will be located inside a directory named `ASTREAM_LOGS` in your home directory on the “romeo” node. Use\n",
    "\n",
    "``` bash\n",
    "ls ~/ASTREAM_LOGS\n",
    "```\n",
    "\n",
    "to find these. We will copy the file associated with *this* experiment to `~/ASTREAM_LOGS/DASH_BUFFER_LOG_last.csv` with\n",
    "\n",
    "``` bash\n",
    "cp $(ls -t1  ~/ASTREAM_LOGS/DASH_BUFFER_LOG_*  | head -n 1 ) ~/ASTREAM_LOGS/DASH_BUFFER_LOG-last.csv\n",
    "```\n",
    "\n",
    "Also reconstruct the video that was delivered to the client. Use\n",
    "\n",
    "``` bash\n",
    "suffix=$(ls -lt | grep \"TEMP_\" | head -n 1 | cut -f2 -d\"_\")\n",
    "cd ~/TEMP_$suffix\n",
    "rm -f ~/BigBuckBunny.mp4 # if it exists\n",
    "cat BigBuckBunny_4s_init.mp4 $(ls -vx BigBuckBunny_*.m4s) > BigBuckBunny_tmp.mp4\n",
    "ffmpeg -i  BigBuckBunny_tmp.mp4 -c copy ~/BigBuckBunny.mp4\n",
    "```\n",
    "\n",
    "to combine the video segments into a `BigBuckBunny.mp4` file in your home directory.\n",
    "\n",
    "Then, you can repeat the data analysis steps as before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "After you have run the experiment, answer the following questions:\n",
    "\n",
    "-   In constant bit rate experiment, did you observe any buffering in any of the DASH policies? Explain.\n",
    "-   In constant bit rate with interruptions experiment, did you see any buffering in basic, netflix and sara policies? Explain with the screenshots from your analysis. Also comment on how well each policy recovers from the buffering and which policy is most resilient to interruptions.\n",
    "-   In varying bit rate experiment, which of the DASH policy has better average video rate? Justify using the screenshots.\n",
    "\n",
    "The following questions requires you to modify certain parameters in each of the experiment:\n",
    "\n",
    "-   What happens when you increase the network capacity from 1Mbps to 4Mbps in constant bit rate experiment?\n",
    "\n",
    "Go to the client node rome and change the current directory to edit the config_dash.py file by running the following commands:\n",
    "\n",
    "``` bash\n",
    "cd ~/AStream/dist/client\n",
    "nano config_dash.py\n",
    "```\n",
    "\n",
    "Under the SARA section, change ALPHA_BUFFER_COUNT to 10 and BETA_BUFFER_COUNT to 15 and Ctrl+X, 'Y' for Yes and 'Enter' to save the changes and exit the file. Now perform the constant bit rate using SARA policy.\n",
    "\n",
    "Note: Revert the configuration changes back to initial settings(ALPHA_BUFFER_COUNT = 5 and BETA_BUFFER_COUNT = 10) by editing config_dash.py file after analysing the logs.\n",
    "\n",
    "-   Observe and explain how changing the buffer thresholds effects different metrics such as average video rate and number of bitrate switching events?\n",
    "\n",
    "Perform varying bit rate experiment with scaling factor of 0.25 for each DASH policy. On router node, run:\n",
    "\n",
    "``` bash\n",
    "bash rate-vary.sh ~/Dataset_1/Dataset/Ferry/Ferry5.csv 0.25\n",
    "```\n",
    "\n",
    "-   What differences do you see in terms of different metrics by changing the scaling factor from 0.1 to 0.25 for each DASH policy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write your own adaptive video policy\n",
    "\n",
    "Do you think you can improve on these adaptive video policies?\n",
    "\n",
    "To write you own adaptive video policy, open basic_dash2.py file that was cloned to your jupyter environment by clicking this link:\n",
    "\n",
    "[AStream/dist/client/adaptation/basic_dash2.py](AStream/dist/client/adaptation/basic_dash2.py)\n",
    "\n",
    "Based on your observations from before, make your own changes to this policy and save it.\n",
    "\n",
    "SCP locally modified policy file to your client romeo such that it replaces previous basic_dash2.py file inside adaptive-video/AStream/dist/client/adaptation/ location.\n",
    "\n",
    "Re-run constant bit rate, constant bit rate with interruption and mobile user experiments to analyze the performance of your policy."
   ]
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "kernelspec": {
   "name": "python",
   "display_name": "Python 3"
  }
 }
}
