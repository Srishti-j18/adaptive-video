{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adaptive video\n",
    "\n",
    "*Fraida Fund ffund@nyu.edu*\n",
    "\n",
    "This experiment explores the tradeoff between different metrics of video quality (average rate, interruptions, and variability of rate) in an adaptive video delivery system.\n",
    "\n",
    "It should take about 60-120 minutes to run this experiment.\n",
    "\n",
    "**Preqreuisites**: To run this experiment on [FABRIC](https://fabric-testbed.net/), you should have a FABRIC account and be part of a FABRIC project. You should have already configured your JupyterHub environment in FABRIC following the instructions [here](https://github.com/fabric-testbed/jupyter-examples/blob/master/fabric_examples/fablib_api/configure_environment/configure_environment.ipynb).\n",
    "\n",
    "## Background\n",
    "\n",
    "### Adaptive video\n",
    "\n",
    "In general high-quality video requires a higher data rate than a lower-quality equivalent. Consider the following two video frames. The first shows a video encoded at 200kbps:\n",
    "\n",
    "![](https://witestlab.poly.edu/blog/content/images/2016/02/dash-200.png)\n",
    "\n",
    "Here’s the same frame at 500kbps, with noticeably better quality:\n",
    "\n",
    "![](https://witestlab.poly.edu/blog/content/images/2016/02/dash-500.png)\n",
    "\n",
    "For web services that want to share video with their users, this poses a dilemma - what quality level should they use to encode the video? If a video is low quality, it will stream without interruption even on a slow 3G cellular connection, but a user on a high speed fiber network may be unhappy with the video quality. Or, the video may be high quality, but then the slow connection would not be able to stream it without constant interruptions.\n",
    "\n",
    "Fortunately, there is a solution to this dilemma: adaptive video. Instead of delivering exactly the same video to every user, adaptive video delivers video that is matched to the individual user’s network quality.\n",
    "\n",
    "There are many different adaptive video products: Microsoft Smooth Streaming, Apple HTTP Live Streaming (HLS), Adobe HTTP Dynamic Streaming (HDS), and Dynamic Adaptive Streaming over HTTP (DASH). This experiment focuses on DASH, which is widely supported as an international standard.\n",
    "\n",
    "To prepare a video for adaptive video streaming with DASH, the video file is first encoded into different versions, each having a different rate and/or resolution. These are called *representations* or media presentations. The representations of a video all have the same content, but they differ in quality.\n",
    "\n",
    "Each of these is further subdivided in time into *segments* of equal lengths (e.g., four seconds).\n",
    "\n",
    "![](https://witestlab.poly.edu/blog/content/images/2016/02/dash-stored.png)\n",
    "\n",
    "The content server then stores all of the segments of all of the representations (as separate files). Alongside these files, the content server stores a manifest file, called the Media Presentation Description (MPD). This is an XML file that identifies the various representations, identifies the video resolution and playback rate for each, and gives the location of every segment in each representation.\n",
    "\n",
    "With these preparations complete, a user can begin to stream adaptive video from the server!\n",
    "\n",
    "Once the MPD and video files are in place, users can start requesting DASH video.\n",
    "\n",
    "First, the user requests the MPD file. It parses the MPD file, learns what representations are available, and decides what representation to request for the first segment. It then retrieves that specific file using the URL given in the MPD.\n",
    "\n",
    "The user’s device keeps a video buffer (at the application layer). As new segments are retrieved, they are placed in the buffer. As video is played back, it is removed from the buffer.\n",
    "\n",
    "Each time a client finishes retrieving a file into the buffer, it makes a new decision as to what representation to get for the next segment.\n",
    "\n",
    "For example, the client might request the following representations for the first four segments of video:\n",
    "\n",
    "![](https://witestlab.poly.edu/blog/content/images/2016/02/dash-requested.png)\n",
    "\n",
    "The cumulative set of decisions made by the client is called a decision policy. The decision policy is a set of rules that determine which representation to request, based on some kind of client state - for example, what the current download rate is, or how much video is currently stored in the buffer.\n",
    "\n",
    "The decision policy is not specified in the DASH standard. Many decision policies have been proposed by researchers, each promising to deliver better quality than the next!\n",
    "\n",
    "### DASH decision policies\n",
    "\n",
    "The obvious policy to maximize video quality alone would be to always retrive segments at the highest quality level. However, with this policy the user is likely to experience rebuffering - when playback is interrupted and the user has to wait for more video to be downloaded. This occurs when the video is being played back (and therefore, removed from the buffer) faster than it is being retrieved - i.e., the playback rate is higher than the download rate - so the buffer becomes empty. This state, which is known as buffer starvation, is obviously something we wish very much to avoid.\n",
    "\n",
    "To create a positive user experience for streaming video, therefore, requires a delicate balancing act.\n",
    "\n",
    "-   On the one hand, increasing the video playback rate too much (so that it is higher than the download rate) causes the undesired rebuffers.\n",
    "-   On the other hand, decreasing the video playback rate also decreases the user-perceived video quality.\n",
    "\n",
    "Performing rate selection to balance rebuffer avoidance and quality optimization is an ongoing tradeoff. Different DASH policies may make different decisions about how to balance that tradeoff. Different DASH policies may also decide to use different pieces of information for decision making. For example:\n",
    "\n",
    "-   A decision policy may decide to focus on download rate in its decision making - select the quality level for the next video segment according to the download rate from the previous segment(s).\n",
    "-   Or, a decision policy may focus on buffer occupancy (how much video is already downloaded into the buffer, waiting to be played back?) If there is already a lot of video in the buffer, the decision policy can afford to be aggressive in its quality selection, since it has a cushion to protect it from rebuffering. On the other hand, if there is not much video in the buffer, the decision policy should be careful not to select a quality level that is too optimistic, since it is at high risk of rebuffering.\n",
    "\n",
    "### Specific policies in this implementation\n",
    "\n",
    "In this experiment, we will use an updated version of the DASH implementation developed for the following paper:\n",
    "\n",
    "> P. Juluri, V. Tamarapalli and D. Medhi, “SARA: Segment aware rate adaptation algorithm for dynamic adaptive streaming over HTTP,” 2015 IEEE International Conference on Communication Workshop (ICCW), 2015, pp. 1765-1770, doi: 10.1109/ICCW.2015.7247436.\n",
    "\n",
    "which you can browse on [Github](https://github.com/teaching-on-testbeds/AStream). It includes three DASH decision policies:\n",
    "\n",
    "The “basic” policy is a rate-based policy that tries to keep the video rate at or below the current network data rate. You can see [the “basic” implementation here](https://github.com/teaching-on-testbeds/AStream/blob/master/dist/client/adaptation/basic_dash2.py).\n",
    "\n",
    "The buffer-based rate adaptation (“netflix”) algorithm uses the estimated network data rate only during the initial startup phase. Otherwise, it makes quality decisions based on the buffer occupancy, i.e. tring to avoid an empty buffer which would cause the video to freeze. It is based on the algorithm described in the following paper:\n",
    "\n",
    "> Te-Yuan Huang, Ramesh Johari, and Nick McKeown. 2013. Downton abbey without the hiccups: buffer-based rate adaptation for HTTP video streaming. In Proceedings of the 2013 ACM SIGCOMM workshop on Future human-centric multimedia networking (FhMN ’13). Association for Computing Machinery, New York, NY, USA, 9–14. https://doi.org/10.1145/2491172.2491179\n",
    "\n",
    "You can see [the “Netflix” implementation here](https://github.com/teaching-on-testbeds/AStream/blob/master/dist/client/adaptation/netflix_dash.py).\n",
    "\n",
    "Finally, the segment-aware rate adaptation (“SARA”) algorithm uses the actual size of the segment and data rate of the network to estimate the time it would take to download the next segment. Then, given the current buffer occupancy, it selects the best possible video quality while avoiding buffer starvation. It is described in\n",
    "\n",
    "> P. Juluri, V. Tamarapalli and D. Medhi, “SARA: Segment aware rate adaptation algorithm for dynamic adaptive streaming over HTTP,” 2015 IEEE International Conference on Communication Workshop (ICCW), 2015, pp. 1765-1770, doi: 10.1109/ICCW.2015.7247436.\n",
    "\n",
    "You can see [the “SARA” implementation here](https://github.com/teaching-on-testbeds/AStream/blob/master/dist/client/adaptation/weighted_dash.py)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reserve resources on FABRIC\n",
    "\n",
    "In this section, we’ll reserve and configure resources on the FABRIC testbed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fabrictestbed_extensions.fablib.fablib import FablibManager as fablib_manager\n",
    "fablib = fablib_manager() \n",
    "conf = fablib.show_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define FABRIC configuration for adaptive video experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slice_name=\"adaptive-video-\" + fablib.get_bastion_username()\n",
    "\n",
    "node_conf = [\n",
    " {'name': \"romeo\",   'cores': 2, 'ram': 4, 'disk': 10, 'image': 'default_ubuntu_20', 'packages': ['net-tools', 'iperf3', 'moreutils']}, \n",
    " {'name': \"juliet\",  'cores': 2, 'ram': 4, 'disk': 10, 'image': 'default_ubuntu_20', 'packages': ['net-tools', 'iperf3', 'moreutils']}, \n",
    " {'name': \"router\",  'cores': 2, 'ram': 4, 'disk': 10, 'image': 'default_ubuntu_20', 'packages': ['net-tools']}\n",
    "]\n",
    "net_conf = [\n",
    " {\"name\": \"net_r\", \"subnet\": \"10.10.1.0/24\", \"nodes\": [{\"name\": \"romeo\",  \"addr\": \"10.10.1.100\"}, {\"name\": \"router\", \"addr\": \"10.10.1.10\"}]},\n",
    " {\"name\": \"net_j\", \"subnet\": \"10.10.2.0/24\", \"nodes\": [{\"name\": \"juliet\", \"addr\": \"10.10.2.100\"}, {\"name\": \"router\", \"addr\": \"10.10.2.10\"}]},\n",
    "]\n",
    "route_conf = [\n",
    " {\"addr\": \"10.10.1.0/24\", \"gw\": \"10.10.2.10\", \"nodes\": [\"juliet\"]}, \n",
    " {\"addr\": \"10.10.2.0/24\", \"gw\": \"10.10.1.10\", \"nodes\": [\"romeo\"]}\n",
    "]\n",
    "exp_conf = {'cores': sum([ n['cores'] for n in node_conf]), 'nic': sum([len(n['nodes']) for n in net_conf]) }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reserve resources\n",
    "\n",
    "Now, we are ready to reserve resources!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, make sure you don’t already have a slice with this name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    slice = fablib.get_slice(slice_name)\n",
    "    print(\"You already have a slice by this name!\")\n",
    "    print(\"If you previously reserved resources, skip to the 'log in to resources' section.\")\n",
    "except:\n",
    "    print(\"You don't have a slice named %s yet.\" % slice_name)\n",
    "    print(\"Continue to the next step to make one.\")\n",
    "    slice = fablib.new_slice(name=slice_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will select a random site that has sufficient resources for our experiment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    site_name = fablib.get_random_site()\n",
    "    if ( (fablib.resources.get_core_available(site_name) > 1.2*exp_conf['cores']) and\n",
    "        (fablib.resources.get_component_available(site_name, 'SharedNIC-ConnectX-6') > 1.2**exp_conf['nic']) ):\n",
    "        break\n",
    "\n",
    "fablib.show_site(site_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we will add hosts and network segments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this cell sets up the nodes\n",
    "for n in node_conf:\n",
    "    slice.add_node(name=n['name'], site=site_name, \n",
    "                   cores=n['cores'], \n",
    "                   ram=n['ram'], \n",
    "                   disk=n['disk'], \n",
    "                   image=n['image'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this cell sets up the network segments\n",
    "for n in net_conf:\n",
    "    ifaces = [slice.get_node(node[\"name\"]).add_component(model=\"NIC_Basic\", \n",
    "                                                 name=n[\"name\"]).get_interfaces()[0] for node in n['nodes'] ]\n",
    "    slice.add_l2network(name=n[\"name\"], type='L2Bridge', interfaces=ifaces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell submits our request to the FABRIC site. The output of this cell will update automatically as the status of our request changes.\n",
    "\n",
    "-   While it is being prepared, the “State” of the slice will appear as “Configuring”.\n",
    "-   When it is ready, the “State” of the slice will change to “StableOK”.\n",
    "\n",
    "You may prefer to walk away and come back in a few minutes (for simple slices) or a few tens of minutes (for more complicated slices with many resources)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slice.submit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slice.get_state()\n",
    "slice.wait_ssh(progress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure resources\n",
    "\n",
    "Next, we will configure the resources so they are ready to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slice = fablib.get_slice(name=slice_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install packages\n",
    "# this will take a while and will run in background while you do other steps\n",
    "for n in node_conf:\n",
    "    if len(n['packages']):\n",
    "        node = slice.get_node(n['name'])\n",
    "        pkg = \" \".join(n['packages'])\n",
    "        node.execute_thread(\"sudo apt update; sudo apt -y install %s\" % pkg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bring interfaces up and either assign an address (if there is one) or flush address\n",
    "from ipaddress import ip_address, IPv4Address, IPv4Network\n",
    "\n",
    "for net in net_conf:\n",
    "    for n in net['nodes']:\n",
    "        if_name = n['name'] + '-' + net['name'] + '-p1'\n",
    "        iface = slice.get_interface(if_name)\n",
    "        iface.ip_link_up()\n",
    "        if n['addr']:\n",
    "            iface.ip_addr_add(addr=n['addr'], subnet=IPv4Network(net['subnet']))\n",
    "        else:\n",
    "            iface.get_node().execute(\"sudo ip addr flush dev %s\"  % iface.get_device_name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare a \"hosts\" file that has names and addresses of every node\n",
    "hosts_txt = [ \"%s\\t%s\" % ( n['addr'], n['name'] ) for net in net_conf  for n in net['nodes'] if type(n) is dict and n['addr']]\n",
    "for n in slice.get_nodes():\n",
    "    for h in hosts_txt:\n",
    "        n.execute(\"echo %s | sudo tee -a /etc/hosts\" % h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enable IPv4 forwarding on all nodes\n",
    "for n in slice.get_nodes():\n",
    "    n.execute(\"sudo sysctl -w net.ipv4.ip_forward=1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up static routes\n",
    "for rt in route_conf:\n",
    "    for n in rt['nodes']:\n",
    "        slice.get_node(name=n).ip_route_add(subnet=IPv4Network(rt['addr']), gateway=rt['gw'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enable nodes to access IPv4-only resources, such as Github,\n",
    "# even if the control interface is IPv6-only\n",
    "from ipaddress import ip_address, IPv6Address\n",
    "for node in slice.get_nodes():\n",
    "    if type(ip_address(node.get_management_ip())) is IPv6Address:\n",
    "        node.execute('echo \"DNS=2a00:1098:2c::1\" | sudo tee -a /etc/systemd/resolved.conf')\n",
    "        node.execute('sudo service systemd-resolved restart')\n",
    "        node.execute('echo \"127.0.0.1 $(hostname -s)\" | sudo tee -a /etc/hosts')\n",
    "        node.execute('sudo rm -f /etc/resolv.conf; sudo ln -sv /run/systemd/resolve/resolv.conf /etc/resolv.conf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Draw the network topology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell will draw the network topology, for your reference. The interface name and addresses of each experiment interface will be shown on the drawing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2_nets = [(n.get_name(), {'color': 'lavender'}) for n in slice.get_l2networks() ]\n",
    "l3_nets = [(n.get_name(), {'color': 'pink'}) for n in slice.get_l3networks() ]\n",
    "hosts   =   [(n.get_name(), {'color': 'lightblue'}) for n in slice.get_nodes()]\n",
    "nodes = l2_nets + l3_nets + hosts\n",
    "ifaces = [iface.toDict() for iface in slice.get_interfaces()]\n",
    "edges = [(iface['network'], iface['node'], \n",
    "          {'label': iface['physical_dev'] + '\\n' + iface['ip_addr'] + '\\n' + iface['mac']}) for iface in ifaces]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(len(nodes),len(nodes)))\n",
    "G = nx.Graph()\n",
    "G.add_nodes_from(nodes)\n",
    "G.add_edges_from(edges)\n",
    "pos = nx.spring_layout(G)\n",
    "nx.draw(G, pos, node_shape='s',  \n",
    "        node_color=[n[1]['color'] for n in nodes], \n",
    "        node_size=[len(n[0])*400 for n in nodes],  \n",
    "        with_labels=True);\n",
    "nx.draw_networkx_edge_labels(G,pos,\n",
    "                             edge_labels=nx.get_edge_attributes(G,'label'),\n",
    "                             font_color='gray',  font_size=8, rotate=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log into resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are finally ready to log in to our resources over SSH! Run the following cells, and observe the table output - you will see an SSH command for each of the resources in your topology."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "slice_info = [{'Name': n.get_name(), 'SSH command': n.get_ssh_command()} for n in slice.get_nodes()]\n",
    "pd.DataFrame(slice_info).set_index('Name')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you can open an SSH session on any of the resources as follows:\n",
    "\n",
    "-   in Jupyter, from the menu bar, use File \\> New \\> Terminal to open a new terminal.\n",
    "-   copy an SSH command from the table, and paste it into the terminal. (Note that each SSH command is a single line, even if the display wraps the text to a second line! When you copy and paste it, paste it all together.)\n",
    "\n",
    "You can repeat this process (open several terminals) to start a session on each resource. Each terminal session will have a tab in the Jupyter environment, so that you can easily switch between them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the adaptive video experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we’re going to install software and set up the materials we need specifically to transfer adaptive video across this network! You will do this by opening SSH sessions to each of the hosts in the topology, and running commands to set them up as needed.\n",
    "\n",
    "Make sure you have the SSH commands ready for each of the hosts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the server\n",
    "\n",
    "First, we will set up the “juliet” host as an adaptive video server. Open an SSH session on “juliet”, and run the commands in this section there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the server, we will set up an HTTP server which will serve the video files to the client.\n",
    "\n",
    "First, install the Apache HTTP server:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` bash\n",
    "sudo apt update  \n",
    "sudo apt install -y apache2  \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, download the video segments and put them in the web server directory. This step will take some time - while it is running, you can open another tab and move on to configuration of the other hosts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` bash\n",
    "wget https://nyu.box.com/shared/static/d6btpwf5lqmkqh53b52ynhmfthh2qtby.tgz -O media.tgz\n",
    "sudo tar -v -xzf media.tgz -C /var/www/html/\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The web server directory now contains 4-second segments of the “open” video clip [Big Buck Bunny](https://peach.blender.org/about/), encoded at different quality levels. The Big Buck Bunny DASH dataset is from:\n",
    "\n",
    "> Stefan Lederer, Christopher Müller, and Christian Timmerer. 2012. Dynamic adaptive streaming over HTTP dataset. In Proceedings of the 3rd Multimedia Systems Conference (MMSys ’12). Association for Computing Machinery, New York, NY, USA, 89–94. DOI:https://doi.org/10.1145/2155555.2155570"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the router\n",
    "\n",
    "Next, we will set up the router. Open an SSH session on “router”, and run the commands in this section there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the router, we will *emulate* different network conditions, to see how each DASH policy performs.\n",
    "\n",
    "We will experiment with both a constant data rate, and a variable data rate like that experienced by a mobile user. For the mobile user, we’ll use some network traces collected in the New York City metro area. With these traces, the data rate experienced by the DASH client in our experiment will mimic the experience of traveling around NYC on bus, subway, and ferry.\n",
    "\n",
    "The NYC traces are shared from the following paper:\n",
    "\n",
    "> Lifan Mei, Runchen Hu, Houwei Cao, Yong Liu, Zifa Han, Feng Li & Jin Li. (2019, March). Realtime Mobile Bandwidth Prediction using LSTM Neural Networks. In International Conference on Passive and Active Network Measurement. Springer.\n",
    "\n",
    "To download the traces, on the “router” node run:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` bash\n",
    "git clone https://github.com/NYU-METS/Main nyc-traces\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To extract the trace files from their compressed archive, we will need to install an appropriate utility:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` bash\n",
    "sudo apt update\n",
    "sudo apt install -y unrar-free\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` bash\n",
    "unrar nyc-traces/Dataset/Dataset_1.rar\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also download a couple of utility scripts to help us set a constant data rate or vary the data rate on the network. On the “router” node, run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` bash\n",
    "wget https://raw.githubusercontent.com/teaching-on-testbeds/adaptive-video/main/rate-vary.sh -O ~/rate-vary.sh\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` bash\n",
    "wget https://raw.githubusercontent.com/teaching-on-testbeds/adaptive-video/main/rate-set.sh -O ~/rate-set.sh\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the client\n",
    "\n",
    "Finally, we need to prepare the “romeo” host as a video client. Open an SSH session on “romeo”, and run the commands in this section there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We must install Python3 to run the DASH video client, and we will also install the video encoding utility `ffmpeg` so that we can reconstruct the video later:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` bash\n",
    "sudo apt update\n",
    "sudo apt install -y python3 ffmpeg\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to run our experiments! We will run four experiments: one with a constant bit rate, one with a constant bit rate and an interruption in middle, one where we compare adaptive video policies under similar network conditions, and one with a varying bit rate using the NYC traces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute constant bit rate experiment\n",
    "\n",
    "For this section, you will need an SSH session on the “router” node and one on the “romeo” node.\n",
    "\n",
    "On the “router”, set a constant bit rate of 1000 Kbits/second with\n",
    "\n",
    "``` bash\n",
    "bash rate-set.sh 1000Kbit\n",
    "```\n",
    "\n",
    "(The first time you run it, you may see an error referencing a problem deleting a `qdisc`, but you can safely ignore this error.)\n",
    "\n",
    "Note: you can specify a data rate in Kbits/second using `Kbit` or in Mbits/second using `Mbit`.\n",
    "\n",
    "Then, on the client (“romeo”), start the DASH player with the “basic” adaptation policy:\n",
    "\n",
    "``` bash\n",
    "python3 ~/AStream/dist/client/dash_client.py -m http://192.168.1.2/media/BigBuckBunny/4sec/BigBuckBunny_4s.mpd -p 'basic' -d\n",
    "```\n",
    "\n",
    "(Note: you can alternatively try `netflix` or `sara` as the DASH policy.)\n",
    "\n",
    "Leave this running for a while. Then, you can interrupt the DASH client with Ctrl+C.\n",
    "\n",
    "To understand the performance of the DASH policy, we can look at the logs produced by the client. These will be located inside a directory named `ASTREAM_LOGS` in your home directory on the “romeo” node. Use\n",
    "\n",
    "``` bash\n",
    "ls ~/ASTREAM_LOGS\n",
    "```\n",
    "\n",
    "to find these.\n",
    "\n",
    "In the data analysis section, we will use these logs - specifically the one that begins with `DASH_BUFFER_LOG_` - to understand the video adaptation policy that was applied in this experiment. We will copy the file associated with *this* experiment to `~/ASTREAM_LOGS/DASH_BUFFER_LOG_last.csv` with\n",
    "\n",
    "``` bash\n",
    "cp $(ls -t1  ~/ASTREAM_LOGS/DASH_BUFFER_LOG_*  | head -n 1 ) ~/ASTREAM_LOGS/DASH_BUFFER_LOG-last.csv\n",
    "```\n",
    "\n",
    "Also reconstruct the video that was delivered to the client. Use\n",
    "\n",
    "``` bash\n",
    "suffix=$(ls -lt | grep \"TEMP_\" | head -n 1 | cut -f2 -d\"_\")\n",
    "cd ~/TEMP_$suffix\n",
    "rm -f ~/BigBuckBunny.mp4 # if it exists\n",
    "cat BigBuckBunny_4s_init.mp4 $(ls -vx BigBuckBunny_*.m4s) > BigBuckBunny_tmp.mp4\n",
    "ffmpeg -i  BigBuckBunny_tmp.mp4 -c copy ~/BigBuckBunny.mp4\n",
    "```\n",
    "\n",
    "to combine the video segments into a `BigBuckBunny.mp4` file in your home directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data analysis\n",
    "\n",
    "After each experiment run (with different variations in experiment conditions!) we can see the video that was delivered to the client, and also see how the video client made its decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, run this cell to make sure your FABRIC configuration and slice configuration is loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['FABRIC_RC_FILE']=os.environ['HOME']+'/work/fabric_config/fabric_rc'\n",
    "os.environ['FABRIC_BASTION_SSH_CONFIG_FILE']=os.environ['HOME']+'/work/fabric_config/ssh_config'\n",
    "\n",
    "from fabrictestbed_extensions.fablib.fablib import FablibManager as fablib_manager\n",
    "fablib = fablib_manager()                     \n",
    "\n",
    "SLICENAME=fablib.get_bastion_username() + \"_adaptive_video\"\n",
    "slice = fablib.get_slice(name=SLICENAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, the following cell will retrieve the reconstructed video file so that you can play it back inside this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slice.get_node(\"romeo\").download_file(\"/home/fabric/work/BigBuckBunny.mp4\", \"/home/ubuntu/BigBuckBunny.mp4\")\n",
    "from IPython.display import Video\n",
    "Video(\"/home/fabric/work/BigBuckBunny.mp4\", embed=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, you can use the log files to find out how the video client made its decisions. In the following cell, fill in the `DASH_BUFFER_LOG` log file name associated with the instance of *your* experiment that you want to analyze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DASH_BUFFER_LOG=\"DASH_BUFFER_LOG-last.csv\"\n",
    "slice.get_node(\"romeo\").download_file(\"/home/fabric/work/DASH_BUFFER_LOG.csv\", \"/home/ubuntu/ASTREAM_LOGS/\" + DASH_BUFFER_LOG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and use the following cell to create a visualization. In the following plot, the line shows the bit rate of each segment as it is played back over time, and the colored background indicates whether the client is playing video (light cyan) or buffering (light pink)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "c = {'INITIAL_BUFFERING': 'violet', 'PLAY': 'lightcyan', 'BUFFERING': 'lightpink'}\n",
    "\n",
    "dash = pd.read_csv(\"/home/fabric/work/DASH_BUFFER_LOG.csv\")\n",
    "dash = dash.loc[dash.CurrentPlaybackState.isin(c.keys() )]\n",
    "states = pd.DataFrame({'startState': dash.CurrentPlaybackState[0:-2].values, 'startTime': dash.EpochTime[0:-2].values,\n",
    "                        'endState':  dash.CurrentPlaybackState[1:-1].values, 'endTime':   dash.EpochTime[1:-1].values})\n",
    "\n",
    "\n",
    "for index, s in states.iterrows():\n",
    "  plt.axvspan(s['startTime'], s['endTime'],  color=c[s['startState']], alpha=1) \n",
    "\n",
    "plt.plot(dash[dash.Action!=\"Writing\"].EpochTime, dash[dash.Action!=\"Writing\"].Bitrate, 'kx:')\n",
    "plt.title(\"Video rate (bps)\");\n",
    "plt.xlabel(\"Time (s)\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also visualize the buffer occupancy over time. In the following plot, the line shows the number of segments in the buffer over time, and the colored background indicates whether the client is playing video (light cyan) or buffering (light pink). When the buffer occupancy goes to zero, the will have to stop playing in order to retrieve more data into the buffer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "c = {'INITIAL_BUFFERING': 'violet', 'PLAY': 'lightcyan', 'BUFFERING': 'lightpink'}\n",
    "\n",
    "dash = pd.read_csv(\"/home/fabric/work/DASH_BUFFER_LOG.csv\")\n",
    "dash = dash.loc[dash.CurrentPlaybackState.isin(c.keys() )]\n",
    "states = pd.DataFrame({'startState': dash.CurrentPlaybackState[0:-2].values, 'startTime': dash.EpochTime[0:-2].values,\n",
    "                        'endState':  dash.CurrentPlaybackState[1:-1].values, 'endTime':   dash.EpochTime[1:-1].values})\n",
    "\n",
    "\n",
    "for index, s in states.iterrows():\n",
    "  plt.axvspan(s['startTime'], s['endTime'],  color=c[s['startState']], alpha=1) \n",
    "\n",
    "plt.plot(dash[dash.Action!=\"Writing\"].EpochTime, dash[dash.Action!=\"Writing\"].CurrentBufferSize, 'kx:')\n",
    "plt.title(\"Buffer(segments)\");\n",
    "plt.xlabel(\"Time (s)\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute constant bit rate experiment with interruption\n",
    "\n",
    "For this section, you will need an SSH session on the “router” node and one on the “romeo” node.\n",
    "\n",
    "In the experiment with constant bit rate, you may not have experienced any rebuffering.\n",
    "\n",
    "To see how the video client works when there is a temporary interruption in the network, try repeating this experiment, but during the video session, reduce the network data rate to a very low value in middle of the session.\n",
    "\n",
    "On the “router”, set a constant bit rate of 1000 Kbits/second with\n",
    "\n",
    "``` bash\n",
    "bash rate-set.sh 1000Kbit\n",
    "```\n",
    "\n",
    "Then, on the client (“romeo”), start the DASH player with the “basic” adaptation policy:\n",
    "\n",
    "``` bash\n",
    "python3 ~/AStream/dist/client/dash_client.py -m http://192.168.1.2/media/BigBuckBunny/4sec/BigBuckBunny_4s.mpd -p 'basic' -d\n",
    "```\n",
    "\n",
    "Leave this running for a while. Then, on the “router”, reduce the network data rate to 50 Kbits/second:\n",
    "\n",
    "    bash rate-set.sh 50Kbit\n",
    "\n",
    "After some time has elapsed, restore the original data rate.\n",
    "\n",
    "``` bash\n",
    "bash rate-set.sh 1000Kbit\n",
    "```\n",
    "\n",
    "Then, after a little while longer, stop the video client on “romeo” with Ctrl+C.\n",
    "\n",
    "As before, the logs produced by the client will be located inside a directory named `ASTREAM_LOGS` in your home directory on the “romeo” node. Use\n",
    "\n",
    "``` bash\n",
    "ls ~/ASTREAM_LOGS\n",
    "```\n",
    "\n",
    "to find these. We will copy the file associated with *this* experiment to `~/ASTREAM_LOGS/DASH_BUFFER_LOG_last.csv` with\n",
    "\n",
    "``` bash\n",
    "cp $(ls -t1  ~/ASTREAM_LOGS/DASH_BUFFER_LOG_*  | head -n 1 ) ~/ASTREAM_LOGS/DASH_BUFFER_LOG-last.csv\n",
    "```\n",
    "\n",
    "Also reconstruct the video that was delivered to the client. Use\n",
    "\n",
    "``` bash\n",
    "suffix=$(ls -lt | grep \"TEMP_\" | head -n 1 | cut -f2 -d\"_\")\n",
    "cd ~/TEMP_$suffix\n",
    "rm -f ~/BigBuckBunny.mp4 # if it exists\n",
    "cat BigBuckBunny_4s_init.mp4 $(ls -vx BigBuckBunny_*.m4s) > BigBuckBunny_tmp.mp4\n",
    "ffmpeg -i  BigBuckBunny_tmp.mp4 -c copy ~/BigBuckBunny.mp4\n",
    "```\n",
    "\n",
    "to combine the video segments into a `BigBuckBunny.mp4` file in your home directory.\n",
    "\n",
    "Then, you can repeat the data analysis steps as before.\n",
    "\n",
    "## Compare adaptive video policies\n",
    "\n",
    "For this section, you will need an SSH session on the “router” node and one on the “romeo” node.\n",
    "\n",
    "As in the previous experiment, in this experiment we will use a constant bit rate, with a brief interruption. However, we will compare the way two video adaptation policies react to this interruption - we’ll compare a *rate based policy* (`basic`) and a *buffer based policy* (`netflix`) under identical settings (similar “high” network rate, “low” data rate, similar duration of the “high” data rate before the interruption, and and similar duration of the “interruption”).\n",
    "\n",
    "#### Rate-based vs. buffer-based policies\n",
    "\n",
    "The basic rate adaptation policy (`basic`) chooses a video rate based on the observed download rate. It keeps track of the average download time for recent segments, and calculates a running average.\n",
    "\n",
    "If the download rate exceeds the current video rate by some threshold, it may increase the video rate. If the download rate is lower than the current video rate, it decreases the video rate to ensure smooth playback.\n",
    "\n",
    "You can see the source code for the `basic` policy here: [basic_dash2.py](../AStream/dist/client/adaptation/basic_dash2.py)\n",
    "\n",
    "The buffer-based policy (`netflix`) adapts the video rate based on the current buffer occupancy, rather than the current download rate. When there are many segments already buffered, it can increase the video rate; if the buffer occupancy is low and the client is at risk of rebuffering, it must decrease the video rate.\n",
    "\n",
    "You can see the source code for the `netflix` policy here: [netflix_dash.py](../AStream/dist/client/adaptation/netflix_dash.py). This policy is based on the paper:\n",
    "\n",
    "> Te-Yuan Huang, Ramesh Johari, and Nick McKeown. 2013. Downton abbey without the hiccups: buffer-based rate adaptation for HTTP video streaming. In Proceedings of the 2013 ACM SIGCOMM workshop on Future human-centric multimedia networking (FhMN ’13). Association for Computing Machinery, New York, NY, USA, 9–14. https://doi.org/10.1145/2491172.2491179\n",
    "\n",
    "The policy defines two buffer occupancy thresholds: reservoir (defaults to 10%) and cushion (defaults to 90%). If the buffer occupancy is below the reservoir threshold, it selects the minimum video rate to fill the buffer quickly. If the buffer is within the reservoir and cushion range, it selects a video rate using a rate map function that maps buffer occupancy to video rate according to some increasing function. If the buffer occupancy exceeds the cushion threshold, it selects the maximum video rate.\n",
    "\n",
    "A brief explanation of the key variables in this policy follows:\n",
    "\n",
    "-   **Reservoir**: Imagine you have a water tank that supplies water to your house. The “reservoir” in this context is like the water level at the bottom of the tank. It’s the minimum amount of water that needs to be there at all times to ensure a steady and uninterrupted water supply. If the reservoir is too low, you might experience water interruptions. Similarly, in video streaming, the reservoir is the minimum amount of video content that must be stored in the buffer to ensure a smooth playback experience. It’s like having a small reserve of video data to prevent any pauses or disruptions if there are temporary changes in network conditions.\n",
    "-   **Cushion**: Think of a cushion on a couch. It’s a soft layer that provides comfort and support. The “cushion” in this context is like an extra layer of video content stored in the buffer above the reservoir level. It’s there to give extra protection against sudden changes. Just as a cushion absorbs some impact, this cushion of video content absorbs any fluctuations in network performance. In video streaming, the cushion is an additional amount of video data stored in the buffer beyond the reservoir level. It’s like having a padding of video content that helps maintain a consistent and uninterrupted playback experience, even when there are brief variations in network speed.\n",
    "\n",
    "Let’s get started!\n",
    "\n",
    "### Execute the experiment for the rate based policy\n",
    "\n",
    "On the “router”, set a constant bit rate of 5000 Kbits/second with\n",
    "\n",
    "``` bash\n",
    "bash rate-set.sh 5000Kbit\n",
    "```\n",
    "\n",
    "Then, on the client (“romeo”), start the DASH player with the “basic” adaptation policy and start the timer along with:\n",
    "\n",
    "``` bash\n",
    "python3 ~/AStream/dist/client/dash_client.py -m http://192.168.1.2/media/BigBuckBunny/4sec/BigBuckBunny_4s.mpd -p 'basic' -d\n",
    "```\n",
    "\n",
    "Let the DASH player run for 100 seconds. After this duration, on the “router” node, reduce the network data rate to 350 Kbits/second:\n",
    "\n",
    "``` bash\n",
    "bash rate-set.sh 350Kbit\n",
    "```\n",
    "\n",
    "After an additional 75 seconds (175 seconds total runtime so far), restore the original data rate.\n",
    "\n",
    "``` bash\n",
    "bash rate-set.sh 5000Kbit\n",
    "```\n",
    "\n",
    "Then, after 300 second, stop the video client on “romeo” with Ctrl+C.\n",
    "\n",
    "As before, the logs produced by the client will be located inside a directory named `ASTREAM_LOGS` in your home directory on the “romeo” node. Use\n",
    "\n",
    "``` bash\n",
    "ls ~/ASTREAM_LOGS\n",
    "```\n",
    "\n",
    "to find these. We will copy the file associated with *this* experiment to `~/ASTREAM_LOGS/DASH_BUFFER_LOG_last.csv` with\n",
    "\n",
    "``` bash\n",
    "cp $(ls -t1  ~/ASTREAM_LOGS/DASH_BUFFER_LOG_*  | head -n 1 ) ~/ASTREAM_LOGS/DASH_BUFFER_LOG-last.csv\n",
    "```\n",
    "\n",
    "Also reconstruct the video that was delivered to the client. Use\n",
    "\n",
    "``` bash\n",
    "suffix=$(ls -lt | grep \"TEMP_\" | head -n 1 | cut -f2 -d\"_\")\n",
    "cd ~/TEMP_$suffix\n",
    "rm -f ~/BigBuckBunny.mp4 # if it exists\n",
    "cat BigBuckBunny_4s_init.mp4 $(ls -vx BigBuckBunny_*.m4s) > BigBuckBunny_tmp.mp4\n",
    "ffmpeg -i  BigBuckBunny_tmp.mp4 -c copy ~/BigBuckBunny.mp4\n",
    "```\n",
    "\n",
    "to combine the video segments into a `BigBuckBunny.mp4` file in your home directory.\n",
    "\n",
    "Repeat the data analysis steps as before. Save the figures and the reconstructed video for the rate based policy, so that after completing the experiment in the next section with the buffer based policy, you can compare their behavior.\n",
    "\n",
    "### Execute the experiment for the buffer based policy\n",
    "\n",
    "On the “router”, set a constant bit rate of 5000 Kbits/second with\n",
    "\n",
    "``` bash\n",
    "bash rate-set.sh 5000Kbit\n",
    "```\n",
    "\n",
    "Then, on the client (“romeo”), start the DASH player with the “netflix” adaptation policy and start the timer along with:\n",
    "\n",
    "``` bash\n",
    "python3 ~/AStream/dist/client/dash_client.py -m http://192.168.1.2/media/BigBuckBunny/4sec/BigBuckBunny_4s.mpd -p 'netflix' -d\n",
    "```\n",
    "\n",
    "Let the DASH player run for 100 seconds. After this duration, on the “router” node, reduce the network data rate to 350 Kbits/second:\n",
    "\n",
    "``` bash\n",
    "bash rate-set.sh 350Kbit\n",
    "```\n",
    "\n",
    "After an additional 75 seconds (175 seconds total runtime so far), restore the original data rate.\n",
    "\n",
    "``` bash\n",
    "bash rate-set.sh 5000Kbit\n",
    "```\n",
    "\n",
    "Then, after 300 second, stop the video client on “romeo” with Ctrl+C.\n",
    "\n",
    "As before, the logs produced by the client will be located inside a directory named `ASTREAM_LOGS` in your home directory on the “romeo” node. Use\n",
    "\n",
    "``` bash\n",
    "ls ~/ASTREAM_LOGS\n",
    "```\n",
    "\n",
    "to find these. We will copy the file associated with *this* experiment to `~/ASTREAM_LOGS/DASH_BUFFER_LOG_last.csv` with\n",
    "\n",
    "``` bash\n",
    "cp $(ls -t1  ~/ASTREAM_LOGS/DASH_BUFFER_LOG_*  | head -n 1 ) ~/ASTREAM_LOGS/DASH_BUFFER_LOG-last.csv\n",
    "```\n",
    "\n",
    "Also reconstruct the video that was delivered to the client. Use\n",
    "\n",
    "``` bash\n",
    "suffix=$(ls -lt | grep \"TEMP_\" | head -n 1 | cut -f2 -d\"_\")\n",
    "cd ~/TEMP_$suffix\n",
    "rm -f ~/BigBuckBunny.mp4 # if it exists\n",
    "cat BigBuckBunny_4s_init.mp4 $(ls -vx BigBuckBunny_*.m4s) > BigBuckBunny_tmp.mp4\n",
    "ffmpeg -i  BigBuckBunny_tmp.mp4 -c copy ~/BigBuckBunny.mp4\n",
    "```\n",
    "\n",
    "to combine the video segments into a `BigBuckBunny.mp4` file in your home directory.\n",
    "\n",
    "Repeat the data analysis steps as before. Save the figures and the reconstructed video for the buffer based policy, for comparison with the rate based policy in the previous section.\n",
    "\n",
    "### Discussions\n",
    "\n",
    "After analyzing the video rate vs. time and buffer vs. time graphs for both the (`basic`) and (`netflix`) adaptation policies, along with the actual video playback, several key observations can be made.\n",
    "\n",
    "Both policies initially start with increasing video rates as the buffer needs to be filled quickly due to being empty. However, in the rate-based policy, the video rate increases more rapidly compared to the buffer-based policy. In the rate-based policy, once the video rate reaches its maximum value, the video runs smoothly without interruption. On the other hand, in the buffer-based policy, the video rate increases, but small interruptions (not exactly buffering) occur because the video rate has not yet reached its maximum.\n",
    "\n",
    "During the interruption phase, the differences in behavior between the two policies become more apparent. In the rate-based (`basic`) policy, there is a noticeable decrease in the video rate and buffering events. This is attributed to the policy’s focus on adjusting the video rate based on the observed download rate. As the data rate decreases, the policy responds by reducing the buffer occupancy, resulting in buffering. In contrast, the buffer-based (`netflix`) policy experiences no buffering events during interruptions. This is a direct consequence of the policy’s adaptation strategy, which relies on the current buffer occupancy rather than the download rate. The buffer-based policy ensures sufficient buffer to accommodate interruptions, leading to smooth and uninterrupted playback.\n",
    "\n",
    "The buffer-based policy (`netflix`) maintains a consistently higher buffer level throughout the experiment, even during interruptions, where it decreases but not significantly. This is evident from the buffer occupancy graph, which generally remains above the buffer threshold level “reservoir”.Conversely, the rate-based policy (`basic`) exhibits fluctuations in buffer occupancy. It begins with a lower buffer level, and during interruptions, the buffer depletes rapidly, resulting in buffering events and drops in video rate.\n",
    "\n",
    "**Video length**: Both policies were executed for a duration of 300 seconds, yet a notable difference exists in the effective video length delivered to the client. In the buffer-based policy, the delivered video length is closer to the original full video length (approximately 594 seconds), indicating more stable and continuous playback.Conversely, in the rate-based policy, the delivered video length slightly exceeds 300 seconds, highlighting that buffering events and reduced buffer size during interruptions led to incomplete video playback.\n",
    "\n",
    "## Execute experiment with varying bit rate (mobile user)\n",
    "\n",
    "Finally, you can try to experience adaptive video as a mobile user!\n",
    "\n",
    "Repeat the experiment, but instead of setting a constant data rate on the router, you can let it play back a “trace” file with e.g. \n",
    "\n",
    "``` bash\n",
    "bash rate-vary.sh ~/Dataset_1/Dataset/Ferry/Ferry5.csv 0.1\n",
    "```\n",
    "\n",
    "where the first argument is the path to a trace file, and the second argument is a scaling factor greater than 0 but less than 1. (The smaller the scaling factor, the lower the network quality while still preserving the trace dynamics.)\n",
    "\n",
    "The following figure shows the “dynamics” (throughput in Mbps against time) for each of the traces:\n",
    "\n",
    "![](https://witestlab.poly.edu/blog/content/images/2022/04/nyc-traces.png)\n",
    "\n",
    "For some traces, the throughput is always more than enough to steam the video at the highest quality level. For the traces where the throughput is *not* sufficient to stream continuously at the highest quality level, a good decision policy should still be able to smooth over the variation in network quality and deliver high quality video without rebuffering.\n",
    "\n",
    "While playing back a trace on the “router”, on the client (“romeo”), start the DASH player with the “basic” adaptation policy:\n",
    "\n",
    "``` bash\n",
    "python3 ~/AStream/dist/client/dash_client.py -m http://192.168.1.2/media/BigBuckBunny/4sec/BigBuckBunny_4s.mpd -p 'basic' -d\n",
    "```\n",
    "\n",
    "Leave this running for a while. Then, stop the video client on “romeo” with Ctrl+C.\n",
    "\n",
    "As before, the logs produced by the client will be located inside a directory named `ASTREAM_LOGS` in your home directory on the “romeo” node. Use\n",
    "\n",
    "``` bash\n",
    "ls ~/ASTREAM_LOGS\n",
    "```\n",
    "\n",
    "to find these. We will copy the file associated with *this* experiment to `~/ASTREAM_LOGS/DASH_BUFFER_LOG_last.csv` with\n",
    "\n",
    "``` bash\n",
    "cp $(ls -t1  ~/ASTREAM_LOGS/DASH_BUFFER_LOG_*  | head -n 1 ) ~/ASTREAM_LOGS/DASH_BUFFER_LOG-last.csv\n",
    "```\n",
    "\n",
    "Also reconstruct the video that was delivered to the client. Use\n",
    "\n",
    "``` bash\n",
    "suffix=$(ls -lt | grep \"TEMP_\" | head -n 1 | cut -f2 -d\"_\")\n",
    "cd ~/TEMP_$suffix\n",
    "rm -f ~/BigBuckBunny.mp4 # if it exists\n",
    "cat BigBuckBunny_4s_init.mp4 $(ls -vx BigBuckBunny_*.m4s) > BigBuckBunny_tmp.mp4\n",
    "ffmpeg -i  BigBuckBunny_tmp.mp4 -c copy ~/BigBuckBunny.mp4\n",
    "```\n",
    "\n",
    "to combine the video segments into a `BigBuckBunny.mp4` file in your home directory.\n",
    "\n",
    "Then, you can repeat the data analysis steps as before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "After you have run the experiment, answer the following questions:\n",
    "\n",
    "-   In constant bit rate experiment, did you observe any buffering in any of the DASH policies? Explain.\n",
    "-   In constant bit rate with interruptions experiment, did you see any buffering in basic, netflix and sara policies? Explain with the screenshots from your analysis. Also comment on how well each policy recovers from the buffering and which policy is most resilient to interruptions.\n",
    "-   In varying bit rate experiment, which of the DASH policy has better average video rate? Justify using the screenshots.\n",
    "\n",
    "The following questions requires you to modify certain parameters in each of the experiment:\n",
    "\n",
    "-   What happens when you increase the network capacity from 1Mbps to 4Mbps in constant bit rate experiment?\n",
    "\n",
    "Go to the client node rome and change the current directory to edit the config_dash.py file by running the following commands:\n",
    "\n",
    "``` bash\n",
    "cd ~/AStream/dist/client\n",
    "nano config_dash.py\n",
    "```\n",
    "\n",
    "Under the SARA section, change ALPHA_BUFFER_COUNT to 10 and BETA_BUFFER_COUNT to 15 and Ctrl+X, 'Y' for Yes and 'Enter' to save the changes and exit the file. Now perform the constant bit rate using SARA policy.\n",
    "\n",
    "Note: Revert the configuration changes back to initial settings(ALPHA_BUFFER_COUNT = 5 and BETA_BUFFER_COUNT = 10) by editing config_dash.py file after analysing the logs.\n",
    "\n",
    "-   Observe and explain how changing the buffer thresholds effects different metrics such as average video rate and number of bitrate switching events?\n",
    "\n",
    "Perform varying bit rate experiment with scaling factor of 0.25 for each DASH policy. On router node, run:\n",
    "\n",
    "``` bash\n",
    "bash rate-vary.sh ~/Dataset_1/Dataset/Ferry/Ferry5.csv 0.25\n",
    "```\n",
    "\n",
    "-   What differences do you see in terms of different metrics by changing the scaling factor from 0.1 to 0.25 for each DASH policy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write your own adaptive video policy\n",
    "\n",
    "Do you think you can improve on these adaptive video policies?\n",
    "\n",
    "To write you own adaptive video policy, open basic_dash2.py file that was cloned to your jupyter environment by clicking this link:\n",
    "\n",
    "[AStream/dist/client/adaptation/basic_dash2.py](AStream/dist/client/adaptation/basic_dash2.py)\n",
    "\n",
    "Based on your observations from before, make your own changes to this policy and save it.\n",
    "\n",
    "SCP locally modified policy file to your client romeo such that it replaces previous basic_dash2.py file inside adaptive-video/AStream/dist/client/adaptation/ location.\n",
    "\n",
    "Re-run constant bit rate, constant bit rate with interruption and mobile user experiments to analyze the performance of your policy."
   ]
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "kernelspec": {
   "name": "python",
   "display_name": "Python 3"
  }
 }
}
